This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-03-04T21:10:03.354Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
backend/
  db_models/
    db_schema.py
  ml/
    ml_model.py
  scripts/
    db_cleanup.py
    ingest_historical_stats.py
    init_db.py
    test_ingestion.py
  app.py
  db_config.py
  dockerfile
  readme.md
  requirements.txt
frontend/
  components/
    BettingDashboard.jsx
    BettingTabs.jsx
    TopPicks.js
  pages/
    _app.js
    index.js
  styles/
    globals.css
  dockerfile
  package.json
  postcss.config.js
  tailwind.config.js
notes/
  testing/
    nba_api_sandbox.py
  nba_api.md
  pandas.md
  todo.txt
.env.example
.gitignore
docker-compose.yml
readme.md

================================================================
Files
================================================================

================
File: backend/db_models/db_schema.py
================
from sqlalchemy import Column, Integer, String, DateTime, Boolean, UniqueConstraint
from db_config import Base

class Player(Base):
    __tablename__ = 'players'
    __table_args__ = {'extend_existing': True}
    
    id = Column(Integer, primary_key=True)
    player_id = Column(Integer, unique=True)
    full_name = Column(String)
    is_active = Column(Boolean, default=True)

class PlayerStats(Base):
    __tablename__ = 'player_stats'
    __table_args__ = (
        UniqueConstraint('player_id', 'game_id', name='uix_player_game'),
        {'extend_existing': True}
    )

    id = Column(Integer, primary_key=True)
    game_id = Column(String)
    player_id = Column(Integer)
    game_date = Column(DateTime)
    season = Column(String)
    is_home_game = Column(Boolean)
    minutes_played = Column(String)
    points = Column(Integer)
    assists = Column(Integer)
    rebounds = Column(Integer)
    steals = Column(Integer)
    blocks = Column(Integer)
    turnovers = Column(Integer)
    plus_minus = Column(Integer)
    fg_made = Column(Integer)
    fg_attempted = Column(Integer)
    fg3_made = Column(Integer)
    fg3_attempted = Column(Integer)
    ft_made = Column(Integer)
    ft_attempted = Column(Integer)

================
File: backend/ml/ml_model.py
================
from typing import Tuple, Dict, List, Optional, Union
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sqlalchemy.orm import Session
from db_models.db_schema import PlayerStats, Player
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ml_model.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class NBAPredictor(nn.Module):
    def __init__(self, input_size: int = 15, hidden_size: int = 64, output_size: int = 1):
        """
        Neural network model for predicting NBA player performance.
        
        Args:
            input_size: Number of input features (default 15 for player stats)
            hidden_size: Size of hidden layers
            output_size: Size of output layer (1 for regression predictions)
        """
        super(NBAPredictor, self).__init__()
        
        # Define neural network architecture
        self.model = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size // 2, output_size)
        )
        
        # Initialize weights using Xavier initialization
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight)
                nn.init.constant_(m.bias, 0)
        
        # Model training state
        self.is_trained = False
        self.scaler = None  # Will store feature scaler
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the network"""
        return self.model(x)
    
    def train_model(self, 
                   X_train: torch.Tensor, 
                   y_train: torch.Tensor, 
                   epochs: int = 100, 
                   lr: float = 0.001, 
                   batch_size: int = 64) -> Dict[str, List[float]]:
        """
        Train the model using the provided training data.
        
        Args:
            X_train: Training features tensor
            y_train: Training target tensor
            epochs: Number of training epochs
            lr: Learning rate
            batch_size: Batch size for training
            
        Returns:
            Dictionary containing training history
        """
        criterion = nn.MSELoss()
        optimizer = optim.Adam(self.parameters(), lr=lr)
        
        # Create data loaders
        dataset = torch.utils.data.TensorDataset(X_train, y_train)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        history = {"loss": []}
        
        # Training loop
        self.train()  # Set model to training mode
        for epoch in range(epochs):
            running_loss = 0.0
            for inputs, targets in dataloader:
                # Zero the parameter gradients
                optimizer.zero_grad()
                
                # Forward pass
                outputs = self(inputs)
                loss = criterion(outputs, targets)
                
                # Backward pass and optimize
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item()
            
            # Log epoch loss
            epoch_loss = running_loss / len(dataloader)
            history["loss"].append(epoch_loss)
            if epoch % 10 == 0:
                logger.info(f'Epoch {epoch} | Loss: {epoch_loss:.6f}')
        
        self.is_trained = True
        return history
    
    def predict(self, features: torch.Tensor) -> torch.Tensor:
        """
        Make predictions using the trained model.
        
        Args:
            features: Input features tensor
            
        Returns:
            Prediction tensor
        """
        if not self.is_trained:
            raise RuntimeError("Model must be trained before making predictions")
        
        self.eval()  # Set model to evaluation mode
        with torch.no_grad():
            return self(features)
    
    def save_model(self, path: str) -> None:
        """
        Save the model to a file.
        
        Args:
            path: Path to save the model
        """
        model_state = {
            'model_state_dict': self.state_dict(),
            'is_trained': self.is_trained,
        }
        torch.save(model_state, path)
        logger.info(f"Model saved to {path}")
    
    def load_model(self, path: str) -> None:
        """
        Load the model from a file.
        
        Args:
            path: Path to load the model from
        """
        try:
            model_state = torch.load(path)
            self.load_state_dict(model_state['model_state_dict'])
            self.is_trained = model_state['is_trained']
            logger.info(f"Model loaded from {path}")
        except Exception as e:
            logger.error(f"Error loading model: {str(e)}")
            raise


class PlayerPropPredictor:
    """Helper class to manage player prop predictions"""
    
    def __init__(self, model_path: Optional[str] = None):
        """
        Initialize the prop predictor with models for different stat categories.
        
        Args:
            model_path: Optional path to load saved models
        """
        # Create models for different stat categories
        self.models = {
            'points': NBAPredictor(),
            'rebounds': NBAPredictor(),
            'assists': NBAPredictor(),
            'steals': NBAPredictor(),
            'blocks': NBAPredictor(),
            'threes': NBAPredictor()
        }
        
        # Load models if path provided
        if model_path:
            try:
                for stat, model in self.models.items():
                    model.load_model(f"{model_path}/{stat}_model.pt")
            except Exception as e:
                logger.error(f"Failed to load models: {str(e)}")
    
    def prepare_player_features(self, session: Session, player_id: int, n_games: int = 10) -> Dict[str, torch.Tensor]:
        """
        Prepare features for a player based on recent game history.
        
        Args:
            session: Database session
            player_id: NBA player ID
            n_games: Number of recent games to use
            
        Returns:
            Dictionary of feature tensors for each stat category
        """
        try:
            # Query recent games for the player
            recent_games = session.query(PlayerStats).filter_by(player_id=player_id)\
                            .order_by(PlayerStats.game_date.desc())\
                            .limit(n_games).all()
            
            if not recent_games:
                logger.warning(f"No recent games found for player {player_id}")
                return {}
            
            # Extract features for each stat category
            features = {}
            for stat in self.models.keys():
                # Build appropriate features for each stat type
                if stat == 'points':
                    raw_features = [
                        [g.points, g.fg_made, g.fg_attempted, g.fg3_made, g.fg3_attempted, 
                         g.ft_made, g.ft_attempted, g.minutes_played, g.is_home_game, 
                         g.plus_minus, g.season, g.turnovers, g.assists, g.rebounds, g.blocks]
                        for g in recent_games
                    ]
                elif stat == 'rebounds':
                    raw_features = [
                        [g.rebounds, g.points, g.blocks, g.minutes_played, g.is_home_game,
                         g.plus_minus, g.season, g.turnovers, g.assists, g.fg_attempted,
                         g.fg3_attempted, g.ft_attempted, g.fg_made, g.fg3_made, g.ft_made]
                        for g in recent_games
                    ]
                elif stat == 'assists':
                    raw_features = [
                        [g.assists, g.points, g.turnovers, g.minutes_played, g.is_home_game,
                         g.plus_minus, g.season, g.rebounds, g.fg_attempted, g.fg3_attempted,
                         g.ft_attempted, g.fg_made, g.fg3_made, g.ft_made, g.blocks]
                        for g in recent_games
                    ]
                elif stat == 'threes':
                    raw_features = [
                        [g.fg3_made, g.fg3_attempted, g.points, g.minutes_played, g.is_home_game,
                         g.plus_minus, g.season, g.turnovers, g.assists, g.rebounds,
                         g.fg_attempted, g.fg_made, g.ft_attempted, g.ft_made, g.blocks]
                        for g in recent_games
                    ]
                else:  # steals and blocks use similar features
                    raw_features = [
                        [getattr(g, stat), g.points, g.minutes_played, g.is_home_game,
                         g.plus_minus, g.season, g.turnovers, g.assists, g.rebounds,
                         g.fg_attempted, g.fg3_attempted, g.ft_attempted, g.fg_made, 
                         g.fg3_made, g.ft_made]
                        for g in recent_games
                    ]
                
                # Convert to tensor
                features[stat] = torch.tensor(raw_features, dtype=torch.float32)
            
            return features
            
        except Exception as e:
            logger.error(f"Error preparing features: {str(e)}")
            return {}
    
    def predict_prop(self, 
                    stat_category: str, 
                    features: torch.Tensor, 
                    prop_line: float,
                    n_samples: int = 100) -> Tuple[str, float]:
        """
        Predict over/under for a prop with confidence.
        
        Args:
            stat_category: Category of the stat ('points', 'rebounds', etc.)
            features: Tensor of player features
            prop_line: The betting line (e.g., 22.5 points)
            n_samples: Number of Monte Carlo samples for confidence
            
        Returns:
            Tuple of (prediction, confidence)
            prediction is "Over" or "Under"
            confidence is a float between 0 and 1
        """
        if stat_category not in self.models:
            raise ValueError(f"Unknown stat category: {stat_category}")
        
        model = self.models[stat_category]
        if not model.is_trained:
            raise RuntimeError(f"Model for {stat_category} not trained")
        
        # Use Monte Carlo dropout for uncertainty estimation
        model.train()  # Enable dropout for MC sampling
        predictions = []
        
        for _ in range(n_samples):
            with torch.no_grad():
                # Use the most recent game's features
                pred = model(features[0].unsqueeze(0)).item()
                predictions.append(pred)
        
        # Calculate mean prediction and standard deviation
        mean_pred = np.mean(predictions)
        std_dev = np.std(predictions)
        
        # Determine if over or under
        prediction = "Over" if mean_pred > prop_line else "Under"
        
        # Calculate confidence based on distance from line and uncertainty
        z_score = abs(mean_pred - prop_line) / (std_dev + 1e-10)  # Add small epsilon to avoid division by zero
        confidence = min(0.5 + 0.5 * (1 - np.exp(-0.5 * z_score)), 0.95)  # Cap at 95%
        
        return prediction, confidence


def get_prediction_with_confidence(model: NBAPredictor, 
                                  game_data: Dict, 
                                  n_samples: int = 100) -> Tuple[float, float]:
    """
    Get a prediction and confidence score for a game outcome.
    
    Args:
        model: Trained NBAPredictor model
        game_data: Dictionary of game data features
        n_samples: Number of Monte Carlo samples for uncertainty
        
    Returns:
        Tuple of (prediction, confidence)
        prediction is the predicted value
        confidence is a value between 0 and 1
    """
    # Convert game data to tensor
    features = []
    for key in ['team', 'opponent', 'is_home_game', 'season']:
        if key in game_data:
            features.append(game_data[key])
    
    features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)
    
    # Use Monte Carlo dropout for uncertainty estimation
    model.train()  # Enable dropout
    predictions = []
    
    for _ in range(n_samples):
        with torch.no_grad():
            pred = model(features_tensor).item()
            predictions.append(pred)
    
    # Calculate mean and standard deviation
    mean_pred = np.mean(predictions)
    std_dev = np.std(predictions)
    
    # Calculate confidence (lower std_dev = higher confidence)
    confidence = 1.0 / (1.0 + std_dev)
    
    return mean_pred, confidence


# Example usage function
def train_prop_models(session: Session, save_path: str = 'models/'):
    """
    Train models for all prop types using data from the database.
    
    Args:
        session: Database session
        save_path: Directory to save trained models
    """
    import os
    os.makedirs(save_path, exist_ok=True)
    
    prop_predictor = PlayerPropPredictor()
    
    # Training data for each stat category
    for stat in prop_predictor.models.keys():
        logger.info(f"Training model for {stat}...")
        
        # Get training data from database
        # In a real app, you would get this data from your database
        # For demonstration purposes, we'll use random data
        
        # Create random training data
        X_train = torch.rand(1000, 15)  # 1000 samples, 15 features
        y_train = torch.rand(1000, 1)   # 1000 targets
        
        # Train model
        prop_predictor.models[stat].train_model(X_train, y_train, epochs=50)
        
        # Save model
        prop_predictor.models[stat].save_model(f"{save_path}/{stat}_model.pt")
        
    logger.info("All models trained and saved successfully.")

================
File: backend/scripts/db_cleanup.py
================
import sys
from pathlib import Path
import logging
from sqlalchemy import create_engine, text
from sqlalchemy.orm import Session


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('db_cleanup.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

backend_dir = Path(__file__).resolve().parent.parent
sys.path.append(str(backend_dir))


from db_config import engine


def remove_duplicate_entries():
    """
    Removes duplicate game entries from the database.
    Keeps only one entry for each player-game combination.
    """
    with Session(engine) as session:
        try:
            # First, identify duplicates
            find_duplicates_query = text("""
                SELECT player_id, game_id, COUNT(*) as count
                FROM player_stats
                GROUP BY player_id, game_id
                HAVING COUNT(*) > 1
            """)
            
            duplicate_result = session.execute(find_duplicates_query)
            duplicates = [{"player_id": row[0], "game_id": row[1], "count": row[2]} for row in duplicate_result]
            
            if not duplicates:
                logger.info("No duplicates found. Database is clean.")
                return
                
            logger.info(f"Found {len(duplicates)} sets of duplicate entries")
            
            # For each set of duplicates, keep the earliest entry and delete the rest
            total_deleted = 0
            for dup in duplicates:
                # Get all duplicate entries for this player-game combination
                find_entries_query = text(f"""
                    SELECT id FROM player_stats
                    WHERE player_id = {dup['player_id']} AND game_id = '{dup['game_id']}'
                    ORDER BY id ASC
                """)
                
                entries_result = session.execute(find_entries_query)
                entry_ids = [row[0] for row in entries_result]
                
                # Keep the first one (with the lowest ID), delete the rest
                if len(entry_ids) > 1:
                    ids_to_delete = entry_ids[1:]
                    delete_query = text(f"""
                        DELETE FROM player_stats
                        WHERE id IN ({','.join(str(id) for id in ids_to_delete)})
                    """)
                    
                    result = session.execute(delete_query)
                    deleted_count = result.rowcount
                    total_deleted += deleted_count
                    
                    logger.info(f"Deleted {deleted_count} duplicates for player {dup['player_id']}, game {dup['game_id']}")
            
            session.commit()
            logger.info(f"Successfully removed {total_deleted} duplicate entries")
            
            # Verify the cleanup was successful
            verify_query = text("""
                SELECT player_id, game_id, COUNT(*) as count
                FROM player_stats
                GROUP BY player_id, game_id
                HAVING COUNT(*) > 1
            """)
            
            verify_result = session.execute(verify_query)
            remaining_duplicates = [row for row in verify_result]
            
            if remaining_duplicates:
                logger.warning(f"There are still {len(remaining_duplicates)} sets of duplicates remaining")
            else:
                logger.info("All duplicates successfully removed")
                
        except Exception as e:
            logger.error(f"Error cleaning up database: {e}")
            session.rollback()
            raise

def verify_game_counts():
    """
    Verifies that no player has more than 82 games in a regular season.
    """
    with Session(engine) as session:
        try:
            query = text("""
                SELECT p.player_id, p.full_name, ps.season, COUNT(*) as game_count
                FROM players p
                JOIN player_stats ps ON p.player_id = ps.player_id
                GROUP BY p.player_id, p.full_name, ps.season
                ORDER BY game_count DESC
            """)
            
            result = session.execute(query)
            player_seasons = [{"player_id": row[0], "name": row[1], "season": row[2], "count": row[3]} 
                             for row in result]
            
            issues = []
            for ps in player_seasons:
                if ps["count"] > 82:
                    issues.append(ps)
            
            if issues:
                logger.warning(f"Found {len(issues)} player-seasons with more than 82 games:")
                for issue in issues[:10]:
                    logger.warning(f"  {issue['name']} (ID: {issue['player_id']}): {issue['count']} games in {issue['season']}")
            else:
                logger.info("All player-seasons have 82 or fewer games")
                
        except Exception as e:
            logger.error(f"Error verifying game counts: {e}")
            raise

if __name__ == "__main__":
    logger.info("Starting database cleanup process")
    
    remove_duplicate_entries()
    verify_game_counts()
    
    logger.info("Database cleanup process completed")

================
File: backend/scripts/ingest_historical_stats.py
================
import sys
from pathlib import Path
import pandas as pd
import time
from datetime import datetime, timedelta
import logging
import random
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import argparse
import json
import os
from typing import List, Dict, Optional, Union, Tuple, Set
from tqdm import tqdm
import backoff
from sqlalchemy.exc import SQLAlchemyError

# Add the backend directory to the path so we can import from models
backend_dir = Path(__file__).resolve().parent.parent
sys.path.append(str(backend_dir))

from nba_api.stats.static import players
from nba_api.stats.endpoints import leaguegamefinder
from sqlalchemy.orm import Session
from db_models.db_schema import PlayerStats, Player
from db_config import engine

# Configure logging
log_dir = Path(__file__).parent / 'logs'
log_dir.mkdir(exist_ok=True)
log_file = log_dir / f'nba_stats_ingestion_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("NBADataIngestion")

# Create a function decorator to handle retries with exponential backoff
def with_retry(max_attempts=5, initial_wait=2.0, backoff_factor=2.0):
    """Decorator for functions that should be retried with exponential backoff."""
    def decorator(func):
        @backoff.on_exception(
            backoff.expo,
            (requests.exceptions.RequestException, 
             ConnectionError, 
             TimeoutError),
            max_tries=max_attempts,
            factor=backoff_factor,
            base=initial_wait,
            jitter=backoff.full_jitter,
            on_backoff=lambda details: logger.warning(
                f"Retrying {func.__name__} in {details['wait']:.2f}s after {details['tries']} attempts due to {details['exception']}"
            )
        )
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)
        return wrapper
    return decorator

class NBADataIngestion:
    def __init__(self, config=None):
        """
        Initialize the NBA data ingestion module with configurable parameters.
        
        Args:
            config (dict, optional): Configuration parameters for customization
        """
        # Default configuration
        self.default_config = {
            "seasons_to_fetch": 5,  # Number of past seasons to fetch
            "request_delay_min": 1.0,  # Minimum delay between requests in seconds
            "request_delay_max": 3.0,  # Maximum delay between requests in seconds
            "batch_size": 10,  # Number of players to process in a batch before committing
            "data_cache_dir": str(Path(__file__).parent / "cache"),  # Directory to cache API responses
            "user_agents": [  # Rotating user agents to avoid API blocks
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
            ],
            "enable_caching": True,  # Enable data caching to reduce API calls
            "verify_data": True,  # Perform data validation after ingestion
            "clean_duplicates": True,  # Remove duplicate entries after ingestion
            "retry_queue_persistence": True,  # Enable saving/loading the retry queue
            "max_retries": 5,  # Maximum number of retries for failed requests
            "base_wait_time": 300  # Base wait time between retries (seconds)
        }
        
        # Initialize retry queue
        self.retry_queue = []

        # Update with provided config
        self.config = self.default_config.copy()
        if config:
            self.config.update(config)
        
        # Create cache directory if enabled
        if self.config["enable_caching"]:
            os.makedirs(self.config["data_cache_dir"], exist_ok=True)
        
        # Set up current season and past seasons to fetch
        self.current_season = datetime.now().year
        self.seasons = [
            f"{year}-{str(year + 1)[-2:]}" 
            for year in range(self.current_season - self.config["seasons_to_fetch"], self.current_season)
        ]
        
        # Configure retry strategy
        retry_strategy = Retry(
            total=5,
            backoff_factor=2,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )

        # Set up session with retry adapter
        self.http_adapter = HTTPAdapter(max_retries=retry_strategy)
        self.http_session = requests.Session()
        self.http_session.mount("https://", self.http_adapter)
        
        # Set initial user agent
        self.rotate_user_agent()
        
        # Track processed players and games to avoid duplicates
        self.processed_player_ids = set()
        self.processed_game_ids = set()
        
        # Session factory for database connections
        self.db_session = None
        
        # Stats for monitoring
        self.stats = {
            "players_processed": 0,
            "games_processed": 0,
            "api_requests": 0,
            "cache_hits": 0,
            "db_inserts": 0,
            "db_updates": 0,
            "errors": 0,
            "rate_limit_hits": 0,
            "start_time": None,
            "end_time": None,
            "last_update_time": None,
            "last_timer_check": None,
            "timer_interval": 60  # How often to update timer (in seconds)
        }
        
        # Load retry queue if persistence is enabled
        if self.config["retry_queue_persistence"]:
            self.load_retry_queue()
        
        logger.info(f"Initialized NBA Data Ingestion with seasons: {', '.join(self.seasons)}")
        logger.info(f"Configuration: {json.dumps({k: v for k, v in self.config.items() if k != 'user_agents'}, indent=2)}")
    
    def rotate_user_agent(self):
        """Rotate the user agent to avoid API blocks."""
        user_agent = random.choice(self.config["user_agents"])
        # Add specific headers that NBA.com expects
        self.http_session.headers.update({
            'User-Agent': user_agent,
            'Referer': 'https://stats.nba.com/',
            'Origin': 'https://stats.nba.com',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'x-nba-stats-origin': 'stats',
            'x-nba-stats-token': 'true'
        })
        return user_agent
    
    def get_cache_path(self, cache_type: str, identifier: str, season: Optional[str] = None) -> Path:
        """
        Get the path for a cached file.
        
        Args:
            cache_type: Type of cache ('players', 'games')
            identifier: Identifier (player_id, etc.)
            season: Season string if applicable
            
        Returns:
            Path to the cache file
        """
        if season:
            return Path(self.config["data_cache_dir"]) / f"{cache_type}_{identifier}_{season}.json"
        return Path(self.config["data_cache_dir"]) / f"{cache_type}_{identifier}.json"
    
    def save_to_cache(self, data: Union[List, Dict], cache_type: str, identifier: str, season: Optional[str] = None) -> bool:
        """
        Save data to cache file.
        
        Args:
            data: Data to cache
            cache_type: Type of cache ('players', 'games')
            identifier: Identifier (player_id, etc.)
            season: Season string if applicable
            
        Returns:
            Success status
        """
        if not self.config["enable_caching"]:
            return False
            
        try:
            cache_path = self.get_cache_path(cache_type, identifier, season)
            with open(cache_path, 'w') as f:
                json.dump(data, f)
            return True
        except Exception as e:
            logger.warning(f"Failed to cache {cache_type} data: {e}")
            return False
    
    def load_from_cache(self, cache_type: str, identifier: str, season: Optional[str] = None) -> Optional[Union[List, Dict]]:
        """
        Load data from cache if available.
        
        Args:
            cache_type: Type of cache ('players', 'games')
            identifier: Identifier (player_id, etc.)
            season: Season string if applicable
            
        Returns:
            Cached data or None if not available
        """
        if not self.config["enable_caching"]:
            return None
            
        cache_path = self.get_cache_path(cache_type, identifier, season)
        if not cache_path.exists():
            return None
            
        try:
            with open(cache_path, 'r') as f:
                data = json.load(f)
            self.stats["cache_hits"] += 1
            return data
        except Exception as e:
            logger.warning(f"Failed to load cached {cache_type} data: {e}")
            return None
    
    def save_retry_queue(self):
        """Save the retry queue to disk."""
        if not self.retry_queue or not self.config["retry_queue_persistence"]:
            return
        
        try:
            queue_path = Path(self.config["data_cache_dir"]) / "retry_queue.json"
            # Convert datetime objects to strings for JSON serialization
            serializable_queue = []
            for item in self.retry_queue:
                item_copy = item.copy()
                item_copy['last_attempt'] = item_copy['last_attempt'].isoformat()
                serializable_queue.append(item_copy)
                
            with open(queue_path, 'w') as f:
                json.dump(serializable_queue, f)
            logger.info(f"Saved {len(self.retry_queue)} items to retry queue")
        except Exception as e:
            logger.error(f"Failed to save retry queue: {e}")

    def load_retry_queue(self):
        """Load the retry queue from disk if available."""
        queue_path = Path(self.config["data_cache_dir"]) / "retry_queue.json"
        if not queue_path.exists():
            return
            
        try:
            with open(queue_path, 'r') as f:
                serialized_queue = json.load(f)
            
            # Convert string dates back to datetime objects
            for item in serialized_queue:
                item['last_attempt'] = datetime.fromisoformat(item['last_attempt'])
                
            self.retry_queue = serialized_queue
            logger.info(f"Loaded {len(self.retry_queue)} items from retry queue")
        except Exception as e:
            logger.error(f"Failed to load retry queue: {e}")
    
    @with_retry(max_attempts=5, initial_wait=2.0, backoff_factor=2.0)
    def get_active_players(self) -> List[Dict]:
        """
        Get a list of all active NBA players.
        
        Returns:
            List of player dictionaries
        """
        cached_players = self.load_from_cache("players", "active")
        if cached_players:
            logger.info(f"Loaded {len(cached_players)} active players from cache")
            return cached_players
            
        try:
            # Rotate user agent before making request
            user_agent = self.rotate_user_agent()
            logger.info(f"Fetching active players with user agent: {user_agent[:30]}...")
            
            # Add randomness for human-like behavior
            human_like_delay = random.uniform(1.5, 5.0)
            time.sleep(human_like_delay)
            
            self.stats["api_requests"] += 1
            active_players = players.get_active_players()
            
            if active_players:
                logger.info(f"Found {len(active_players)} active players")
                # Cache the results
                self.save_to_cache(active_players, "players", "active")
                return active_players
            else:
                logger.warning("API returned empty player list")
                return []
                
        except requests.exceptions.HTTPError as e:
            if hasattr(e, 'response') and e.response.status_code == 429:
                logger.warning("Rate limit exceeded when fetching active players")
                self.stats["rate_limit_hits"] += 1
            logger.error(f"HTTP error fetching active players: {e}")
            self.stats["errors"] += 1
            return []
        except Exception as e:
            logger.error(f"Error fetching active players: {e}")
            self.stats["errors"] += 1
            return []
    
    @with_retry(max_attempts=5, initial_wait=2.0, backoff_factor=2.0)
    def get_player_games(self, player_id: int, season: str) -> Optional[pd.DataFrame]:
        """
        Get games for a specific player and season.
        
        Args:
            player_id: NBA player ID
            season: Season string (e.g. '2022-23')
            
        Returns:
            DataFrame of player games, empty DataFrame if no games found,
            or None if an error occurred
        """
        # Try to load from cache first
        cached_games = self.load_from_cache("games", player_id, season)
        if cached_games:
            logger.debug(f"Loaded {len(cached_games)} games for player {player_id} in {season} from cache")
            return pd.DataFrame(cached_games)
        
        # Add randomness between season requests for the same player
        human_like_delay = random.uniform(1.5, 7.0)
        time.sleep(human_like_delay)
        
        # Implement exponential backoff with jitter
        delay = self.config["request_delay_min"] + random.uniform(0, self.config["request_delay_max"] - self.config["request_delay_min"])
        time.sleep(delay)
        
        try:
            # Rotate user agent
            user_agent = self.rotate_user_agent()
            logger.debug(f"Fetching games for player {player_id} in {season} with user agent: {user_agent[:30]}...")
            
            self.stats["api_requests"] += 1
            player_games_query = leaguegamefinder.LeagueGameFinder(
                player_or_team_abbreviation="P",
                player_id_nullable=player_id,
                season_nullable=season,
                season_type_nullable="Regular Season",
                timeout=180  # Extended timeout
            )
            
            games_df = player_games_query.get_data_frames()[0]
            
            if not games_df.empty:
                logger.info(f"Successfully retrieved {len(games_df)} games for player {player_id} in {season}")
                
                # Cache the results as a list of dictionaries
                games_data = games_df.to_dict('records')
                self.save_to_cache(games_data, "games", player_id, season)
                
                return games_df
            else:
                # This means we got a valid empty response (player didn't play that season)
                logger.info(f"No games found for player {player_id} in {season}")
                return pd.DataFrame()
                    
        except requests.exceptions.HTTPError as e:
            if hasattr(e, 'response') and e.response.status_code == 429:
                # Rate limit exceeded
                logger.warning(f"Rate limit exceeded for player {player_id} in {season}. Adding to retry queue with longer delay.")
                self.stats["rate_limit_hits"] += 1
                self.stats["errors"] += 1
                return None
            else:
                logger.error(f"HTTP error fetching games for player {player_id} in {season}: {e}")
                self.stats["errors"] += 1
                return None
        except requests.exceptions.Timeout as e:
            # Explicitly handle timeouts differently
            logger.error(f"Timeout error fetching games for player {player_id} in {season}: {e}")
            self.stats["errors"] += 1
            # Return None to indicate an error occurred (not an empty result)
            return None
        except Exception as e:
            logger.error(f"Error fetching games for player {player_id} in {season}: {e}")
            self.stats["errors"] += 1
            # Return None to indicate an error occurred
            return None

    def process_game_data(self, game_data: pd.Series, season: str) -> Dict:
        """
        Format raw game data into a Python Dictionary.
        
        Args:
            game_data: Series containing game statistics
            season: Season string
            
        Returns:
            Dictionary with formatted game data
        """
        try:
            processed_data = {
                'game_id': game_data['GAME_ID'],
                'player_id': game_data['PLAYER_ID'],
                'game_date': pd.to_datetime(game_data['GAME_DATE']),
                'season': season,
                'is_home_game': '@' not in game_data['MATCHUP'],
                'minutes_played': game_data['MIN'],
                'points': game_data['PTS'],
                'rebounds': game_data['REB'],
                'assists': game_data['AST'],
                'steals': game_data['STL'],
                'blocks': game_data['BLK'],
                'turnovers': game_data['TOV'],
                'plus_minus': game_data['PLUS_MINUS'],
                'fg_made': game_data['FGM'],
                'fg_attempted': game_data['FGA'],
                'fg3_made': game_data['FG3M'],
                'fg3_attempted': game_data['FG3A'],
                'ft_made': game_data['FTM'],
                'ft_attempted': game_data['FTA']
            }
            
            # Validate data
            for key, value in processed_data.items():
                if key != 'game_date' and isinstance(value, (int, float)) and pd.isna(value):
                    logger.warning(f"NaN value detected for {key} in game {game_data['GAME_ID']} for player {game_data['PLAYER_ID']}")
                    processed_data[key] = 0
            
            return processed_data
            
        except Exception as e:
            logger.error(f"Error processing game data: {e}")
            self.stats["errors"] += 1
            return None

    def get_or_create_db_session(self):
        """Get an existing db session or create a new one."""
        if self.db_session is None:
            self.db_session = Session(engine)
        return self.db_session
    
    def store_player_data(self, player: Dict) -> bool:
        """
        Store player data in database.
        
        Args:
            player: Player dictionary
            
        Returns:
            Success status
        """
        if player['id'] in self.processed_player_ids:
            logger.debug(f"Player {player['id']} ({player['full_name']}) already processed, skipping")
            return True
            
        session = self.get_or_create_db_session()
        try:
            existing_player = session.query(Player).filter_by(player_id=player['id']).first()
            
            if existing_player:
                # Update existing player
                existing_player.full_name = player['full_name']
                existing_player.is_active = True
                self.stats["db_updates"] += 1
                logger.debug(f"Updated player: {player['full_name']} (ID: {player['id']})")
            else:
                # Create new player
                player_record = Player(
                    player_id=player['id'],
                    full_name=player['full_name'],
                    is_active=True
                )
                session.add(player_record)
                self.stats["db_inserts"] += 1
                logger.debug(f"Added new player: {player['full_name']} (ID: {player['id']})")
            

            # Add to processed set
            self.processed_player_ids.add(player['id'])
            self.stats["players_processed"] += 1
            session.commit()
            
            return True
            
        except SQLAlchemyError as e:
            logger.error(f"Database error storing player {player['full_name']}: {e}")
            session.rollback()
            self.stats["errors"] += 1
            return False
        except Exception as e:
            logger.error(f"Error storing player data for {player['full_name']}: {e}")
            session.rollback()
            self.stats["errors"] += 1
            return False

    def store_game_stats(self, stats_data: Dict) -> bool:
        """
        Insert or update game stats in the database.
        
        Args:
            stats_data: Game statistics dictionary
            
        Returns:
            Success status
        """
        if not stats_data:
            return False
            
        # Check if this game has already been processed
        game_player_key = f"{stats_data['game_id']}_{stats_data['player_id']}"
        if game_player_key in self.processed_game_ids:
            logger.debug(f"Game {game_player_key} already processed, skipping")
            return True
            
        session = self.get_or_create_db_session()
        try:
            # Check if this game already exists
            existing_game = session.query(PlayerStats).filter_by(
                game_id=stats_data['game_id'],
                player_id=stats_data['player_id']
            ).first()
            
            if existing_game:
                # Update fields instead of using merge to avoid potential issues
                for key, value in stats_data.items():
                    if key != 'id':  # Skip primary key
                        setattr(existing_game, key, value)
                self.stats["db_updates"] += 1
            else:
                # Create new game stats record
                stats = PlayerStats(**stats_data)
                session.add(stats)
                self.stats["db_inserts"] += 1
            
            # Add to processed set
            self.processed_game_ids.add(game_player_key)
            self.stats["games_processed"] += 1

            session.commit()
            
            return True
            
        except SQLAlchemyError as e:
            logger.error(f"Database error storing game stats: {e}")
            session.rollback()
            self.stats["errors"] += 1
            return False
        except Exception as e:
            logger.error(f"Error storing game stats: {e}")
            session.rollback()
            self.stats["errors"] += 1
            return False
    
    def commit_batch(self):
        """Commit the current batch of database operations."""
        if self.db_session:
            try:
                self.db_session.commit()
                logger.debug("Committed batch of database operations")
            except SQLAlchemyError as e:
                logger.error(f"Error committing batch: {e}")
                self.db_session.rollback()
                self.stats["errors"] += 1

    def close_session(self):
        """Close the database session."""
        if self.db_session:
            self.db_session.close()
            self.db_session = None
            logger.debug("Closed database session")
    
    def process_player(self, player: Dict) -> Tuple[int, int]:
        """
        Process a single player's data for all seasons.
        
        Args:
            player: Player dictionary
            
        Returns:
            Tuple of (games processed, errors)
        """
        player_name = player['full_name']
        player_id = player['id']
        
        logger.info(f"Processing player {player_name} (ID: {player_id})")
        games_processed = 0
        errors = 0
        
        # Store player info
        if not self.store_player_data(player):
            logger.error(f"Failed to store player data for {player_name}")
            errors += 1
            return games_processed, errors
        
        # Process each season
        for season in self.seasons:
            try:
                logger.info(f"Fetching games for {player_name} in {season}")
                games_df = self.get_player_games(player_id, season)

                if games_df is None:
                    # Add to retry queue
                    self.retry_queue.append({
                        'player_id': player_id,
                        'player_name': player_name,
                        'season': season,
                        'retries': 0,
                        'last_attempt': datetime.now()
                    })
                    logger.warning(f"Added {player_name} in {season} to retry queue due to API errors")
                    continue
                
                if games_df.empty:
                    logger.info(f"No games found for {player_name} in {season}")
                    continue
                
                logger.info(f"Processing {len(games_df)} games for {player_name} in {season}")
                
                # Process each game
                for _, game in games_df.iterrows():
                    try:
                        processed_data = self.process_game_data(game, season)
                        if processed_data:
                            if self.store_game_stats(processed_data):
                                games_processed += 1
                            else:
                                errors += 1
                    except Exception as e:
                        logger.error(f"Error processing game for {player_name}: {e}")
                        errors += 1
                
                logger.info(f"Completed {games_processed} games for {player_name} in {season}")
                
            except Exception as e:
                logger.error(f"Error processing season {season} for {player_name}: {e}")
                errors += 1
                continue
        
        # Commit after each player to save progress
        self.commit_batch()
        
        return games_processed, errors
    
    def calculate_time_remaining(self, processed_count, total_count):
        """
        Calculate and format estimated time remaining for the process.
        
        Args:
            processed_count: Number of items processed so far
            total_count: Total number of items to process
            
        Returns:
            Formatted string with estimated time remaining
        """
        if processed_count == 0:
            return "Calculating..."
            
        current_time = datetime.now()
        elapsed = (current_time - self.stats["start_time"]).total_seconds()
        
        # Calculate time per item and estimate remaining time
        time_per_item = elapsed / processed_count
        items_remaining = total_count - processed_count
        seconds_remaining = time_per_item * items_remaining
        
        # Format remaining time
        remaining_time = timedelta(seconds=int(seconds_remaining))
        hours, remainder = divmod(remaining_time.total_seconds(), 3600)
        minutes, seconds = divmod(remainder, 60)
        
        # Format as HH:MM:SS
        time_str = f"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}"
        
        return time_str
        
    def update_timer(self, processed_count, total_count, force=False):
        """
        Update the timer display if the interval has passed.
        
        Args:
            processed_count: Number of items processed so far
            total_count: Total number of items to process
            force: Force update regardless of timer interval
        """
        current_time = datetime.now()
        
        # Initialize last timer check if needed
        if self.stats["last_timer_check"] is None:
            self.stats["last_timer_check"] = current_time
            return
            
        time_since_check = (current_time - self.stats["last_timer_check"]).total_seconds()
        
        # Update timer if interval passed or forced
        if force or time_since_check >= self.stats["timer_interval"]:
            remaining_str = self.calculate_time_remaining(processed_count, total_count)
            elapsed = (current_time - self.stats["start_time"]).total_seconds()
            hours, remainder = divmod(elapsed, 3600)
            minutes, seconds = divmod(remainder, 60)
            elapsed_str = f"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}"
            
            progress_pct = (processed_count / total_count) * 100 if total_count > 0 else 0
            
            logger.info(f"Progress: {processed_count}/{total_count} players ({progress_pct:.1f}%)")
            logger.info(f"Elapsed time: {elapsed_str} | Time remaining: {remaining_str}")
            logger.info(f"Rate limit hits: {self.stats['rate_limit_hits']} | Retry queue size: {len(self.retry_queue)}")
            
            # Update timer check timestamp
            self.stats["last_timer_check"] = current_time
            
            # Save retry queue periodically
            if self.config["retry_queue_persistence"] and self.retry_queue:
                self.save_retry_queue()
            
            return remaining_str
        
        return None
    
    def process_retry_queue(self, max_retries=None, base_wait_time=None):
        """
        Process items in the retry queue with a sufficient delay between attempts.
        
        Args:
            max_retries: Maximum number of retries (default: from config)
            base_wait_time: Base wait time between retries (default: from config)
        """
        if not self.retry_queue:
            logger.info("Retry queue is empty")
            return
        
        # Use config values if not provided
        max_retries = max_retries or self.config["max_retries"]
        base_wait_time = base_wait_time or self.config["base_wait_time"]
            
        logger.info(f"Processing retry queue with {len(self.retry_queue)} items")
        current_time = datetime.now()
        
        # Copy queue to avoid modification during iteration
        queue_copy = self.retry_queue.copy()
        self.retry_queue = []
        
        for item in queue_copy:
            # Calculate exponential backoff wait time
            required_wait_time = base_wait_time * (2 ** item['retries'])
            time_since_last = (current_time - item['last_attempt']).total_seconds()
            
            if time_since_last < required_wait_time:
                # Not waited long enough, put back in queue
                self.retry_queue.append(item)
                continue
                
            if item['retries'] >= max_retries:
                logger.warning(f"Giving up on {item['player_name']} in {item['season']} after {max_retries} attempts")
                continue
                
            logger.info(f"Retrying {item['player_name']} in {item['season']} (attempt {item['retries'] + 1})")
            
            # Execute with a longer timeout
            try:
                # Rotate user agent
                self.rotate_user_agent()
                
                # Add human-like randomness
                human_like_delay = random.uniform(2.0, 8.0)
                time.sleep(human_like_delay)
                
                # Use a direct API call with longer timeout
                player_games_query = leaguegamefinder.LeagueGameFinder(
                    player_or_team_abbreviation="P",
                    player_id_nullable=item['player_id'],
                    season_nullable=item['season'],
                    season_type_nullable="Regular Season",
                    timeout=300  # Extended timeout for retries
                )
                
                games_df = player_games_query.get_data_frames()[0]
                
                if not games_df.empty:
                    logger.info(f"Retry successful! Retrieved {len(games_df)} games for {item['player_name']} in {item['season']}")
                    
                    # Cache the results
                    games_data = games_df.to_dict('records')
                    self.save_to_cache(games_data, "games", item['player_id'], item['season'])
                    
                    # Process the games
                    games_processed = 0
                    for _, game in games_df.iterrows():
                        processed_data = self.process_game_data(game, item['season'])
                        if processed_data and self.store_game_stats(processed_data):
                            games_processed += 1
                    
                    logger.info(f"Processed {games_processed} games from retry for {item['player_name']} in {item['season']}")
                    
                else:
                    logger.info(f"Retry successful but no games found for {item['player_name']} in {item['season']}")
                    
            except Exception as e:
                # Increment retry count and put back in queue
                item['retries'] += 1
                item['last_attempt'] = datetime.now()
                self.retry_queue.append(item)
                logger.error(f"Retry failed for {item['player_name']} in {item['season']}: {e}")
        
        # Save updated retry queue
        if self.config["retry_queue_persistence"] and self.retry_queue:
            self.save_retry_queue()
            
        logger.info(f"Retry queue processing complete. {len(self.retry_queue)} items remaining")
    
    def run_ingestion(self, start_player=0, end_player=None):
        """
        Run ingestion process for a specific range of players.
        
        Args:
            start_player: Index of the first player to process (0-based)
            end_player: Index of the last player to process (exclusive)
        """
        logger.info("Starting data ingestion")
        logger.info(f"Will collect data for seasons: {', '.join(self.seasons)}")
        logger.info("=" * 80)
        
        self.stats["start_time"] = datetime.now()
        self.stats["last_update_time"] = self.stats["start_time"]
        self.stats["last_timer_check"] = self.stats["start_time"]
        
        # Get active players
        active_players = self.get_active_players()
        
        if not active_players:
            logger.error("Failed to retrieve active players list")
            return
        
        # Apply range selection
        if end_player is not None:
            active_players = active_players[start_player:end_player]
        elif start_player > 0:
            active_players = active_players[start_player:]
        
        # Log the player range being processed
        player_range_info = f"Processing players {start_player}"
        if end_player is not None:
            player_range_info += f" to {end_player-1}"
        logger.info(player_range_info)
        
        total_players = len(active_players)
        logger.info(f"Processing {total_players} players sequentially")
        logger.info(f"Initial timer estimate: {self.calculate_time_remaining(1, total_players)}")
        
        # Track progress for better visibility
        progress = tqdm(total=total_players, desc="Processing players")
        
        # Process each player one at a time
        successful_players = 0
        failed_players = 0
        
        # Add rate limiting tracker
        rate_limit_hits = 0
        consecutive_timeouts = 0
        
        for idx, player in enumerate(active_players, 1):
            # If we've hit rate limits multiple times in succession, take a longer break
            if consecutive_timeouts > 3:
                logger.warning(f"Detected multiple consecutive timeouts. Taking a longer break (120s)")
                time.sleep(120)
                consecutive_timeouts = 0
                
            # Take a longer defensive break every 50 players
            if idx > 0 and idx % 50 == 0:
                cooling_period = random.uniform(180, 300)  # 3-5 minute break
                logger.info(f"Taking a defensive cooling break of {cooling_period:.1f}s after {idx} players")
                time.sleep(cooling_period)
                
            try:
                player_name = player['full_name']
                logger.info(f"Processing player {idx}/{total_players}: {player_name}")
                
                games_processed, errors = self.process_player(player)
                
                if games_processed > 0:
                    successful_players += 1
                    consecutive_timeouts = 0  # Reset timeout counter on success
                    logger.info(f"Successfully processed {games_processed} games for {player_name} with {errors} errors")
                else:
                    # If no games were processed but we didn't hit an exception
                    if errors == 0:
                        logger.info(f"No games found for {player_name}")
                        consecutive_timeouts = 0  # Also reset on valid empty responses
                    else:
                        failed_players += 1
                        consecutive_timeouts += 1
                        logger.warning(f"Failed to process any games for {player_name} with {errors} errors")
                
                # Commit at regular intervals to save progress
                if (idx % self.config["batch_size"]) == 0:
                    logger.info(f"Checkpoint: Processed {idx}/{total_players} players, committing batch")
                    self.commit_batch()
                    
                    # Update timer information
                    self.update_timer(idx, total_players)
                
                # Add random delay between players to avoid API rate limits
                # Use a longer delay if we've recently hit rate limits
                if self.stats["rate_limit_hits"] > rate_limit_hits:
                    # We hit a new rate limit in this iteration
                    logger.warning("Rate limit detected, increasing delay")
                    delay = random.uniform(
                        self.config["request_delay_max"], 
                        self.config["request_delay_max"] * 2
                    )
                    rate_limit_hits = self.stats["rate_limit_hits"]
                else:
                    delay = random.uniform(
                        self.config["request_delay_min"], 
                        self.config["request_delay_max"]
                    )
                
                time.sleep(delay)
                
            except Exception as e:
                failed_players += 1
                consecutive_timeouts += 1
                logger.error(f"Error processing player {player['full_name']}: {str(e)}")
                self.stats["errors"] += 1
            finally:
                # Update progress bar regardless of success/failure
                progress.update(1)
        
        progress.close()
        
        # Final commit for any remaining changes
        self.commit_batch()

        # Process retry queue if there are items
        if self.retry_queue:
            logger.info(f"Processing {len(self.retry_queue)} items in retry queue")
            self.process_retry_queue()
            self.commit_batch()
        
        # Final timer update
        self.update_timer(total_players, total_players, force=True)
        
        logger.info(f"Player processing completed: {successful_players} successful, {failed_players} failed")
        
        # Run cleanup operations if configured
        if self.config["clean_duplicates"]:
            logger.info("Running duplicate cleanup...")
            self.clean_duplicate_entries()
        
        if self.config["verify_data"]:
            logger.info("Verifying data integrity...")
            self.verify_ingested_data()
        
        self.stats["end_time"] = datetime.now()
        self.print_stats()
        
        # Close the database session
        self.close_session()
        
        logger.info("Data ingestion completed successfully")
    
    def clean_duplicate_entries(self):
        """Remove duplicate game entries from the database."""
        logger.info("Cleaning duplicate entries from the database")
        session = self.get_or_create_db_session()
        
        try:
            # First, identify duplicates
            find_duplicates_query = """
                SELECT player_id, game_id, COUNT(*) as count
                FROM player_stats
                GROUP BY player_id, game_id
                HAVING COUNT(*) > 1
            """
            
            duplicate_result = session.execute(find_duplicates_query)
            duplicates = [{"player_id": row[0], "game_id": row[1], "count": row[2]} for row in duplicate_result]
            
            if not duplicates:
                logger.info("No duplicates found. Database is clean.")
                return
                
            logger.info(f"Found {len(duplicates)} sets of duplicate entries")
            
            # For each set of duplicates, keep the earliest entry and delete the rest
            total_deleted = 0
            for dup in duplicates:
                # Get all duplicate entries for this player-game combination
                find_entries_query = f"""
                    SELECT id FROM player_stats
                    WHERE player_id = {dup['player_id']} AND game_id = '{dup['game_id']}'
                    ORDER BY id ASC
                """
                
                entries_result = session.execute(find_entries_query)
                entry_ids = [row[0] for row in entries_result]
                
                # Keep the first one (with the lowest ID), delete the rest
                if len(entry_ids) > 1:
                    ids_to_delete = entry_ids[1:]
                    delete_query = f"""
                        DELETE FROM player_stats
                        WHERE id IN ({','.join(str(id) for id in ids_to_delete)})
                    """
                    
                    result = session.execute(delete_query)
                    deleted_count = result.rowcount
                    total_deleted += deleted_count
                    
                    logger.info(f"Deleted {deleted_count} duplicates for player {dup['player_id']}, game {dup['game_id']}")
            
            session.commit()
            logger.info(f"Successfully removed {total_deleted} duplicate entries")
            
            # Verify the cleanup was successful
            verify_query = """
                SELECT player_id, game_id, COUNT(*) as count
                FROM player_stats
                GROUP BY player_id, game_id
                HAVING COUNT(*) > 1
            """
            
            verify_result = session.execute(verify_query)
            remaining_duplicates = [row for row in verify_result]
            
            if remaining_duplicates:
                logger.warning(f"There are still {len(remaining_duplicates)} sets of duplicates remaining")
            else:
                logger.info("All duplicates successfully removed")
                
        except Exception as e:
            logger.error(f"Error cleaning up database: {e}")
            session.rollback()
    
    def verify_ingested_data(self):
        """Verify the integrity of ingested data."""
        logger.info("Verifying ingested data")
        session = self.get_or_create_db_session()
        
        try:
            # 1. Check for players with more than 82 games in a season
            query = """
                SELECT p.player_id, p.full_name, ps.season, COUNT(*) as game_count
                FROM players p
                JOIN player_stats ps ON p.player_id = ps.player_id
                GROUP BY p.player_id, p.full_name, ps.season
                HAVING COUNT(*) > 82
                ORDER BY game_count DESC
            """
            
            result = session.execute(query)
            issues = [{"player_id": row[0], "name": row[1], "season": row[2], "count": row[3]} 
                    for row in result]
            
            if issues:
                logger.warning(f"Found {len(issues)} player-seasons with more than 82 games:")
                for issue in issues[:10]:  # Show only first 10 to avoid log spam
                    logger.warning(f"  {issue['name']} (ID: {issue['player_id']}): {issue['count']} games in {issue['season']}")
            else:
                logger.info("All player-seasons have 82 or fewer games - data looks valid")
            
            # 2. Check for missing essential data
            missing_data_query = """
                SELECT COUNT(*) FROM player_stats 
                WHERE points IS NULL OR rebounds IS NULL OR assists IS NULL
            """
            missing_count = session.execute(missing_data_query).scalar()
            
            if missing_count > 0:
                logger.warning(f"Found {missing_count} records with missing essential stats")
            else:
                logger.info("No records with missing essential stats - data looks valid")
            
            # 3. Check for players without any game data
            orphaned_query = """
                SELECT p.player_id, p.full_name
                FROM players p
                LEFT JOIN player_stats ps ON p.player_id = ps.player_id
                WHERE ps.id IS NULL
            """
            
            orphan_result = session.execute(orphaned_query)
            orphans = [{"player_id": row[0], "name": row[1]} for row in orphan_result]
            
            if orphans:
                logger.warning(f"Found {len(orphans)} players with no game data:")
                for orphan in orphans[:10]:  # Show only first 10
                    logger.warning(f"  {orphan['name']} (ID: {orphan['player_id']})")
            else:
                logger.info("All players have associated game data - data looks valid")
                
        except Exception as e:
            logger.error(f"Error verifying data: {e}")
    
    def print_stats(self):
        """Print statistics about the ingestion process."""
        if not self.stats["start_time"] or not self.stats["end_time"]:
            logger.warning("Cannot print stats: missing start or end time")
            return
            
        duration = self.stats["end_time"] - self.stats["start_time"]
        hours, remainder = divmod(duration.total_seconds(), 3600)
        minutes, seconds = divmod(remainder, 60)
        
        logger.info("=" * 50)
        logger.info("INGESTION STATISTICS")
        logger.info("=" * 50)
        logger.info(f"Duration: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}")
        logger.info(f"Players processed: {self.stats['players_processed']}")
        logger.info(f"Games processed: {self.stats['games_processed']}")
        logger.info(f"API requests: {self.stats['api_requests']}")
        logger.info(f"Cache hits: {self.stats['cache_hits']}")
        logger.info(f"Database inserts: {self.stats['db_inserts']}")
        logger.info(f"Database updates: {self.stats['db_updates']}")
        logger.info(f"Errors: {self.stats['errors']}")
        logger.info(f"Rate limit hits: {self.stats['rate_limit_hits']}")
        logger.info(f"Items remaining in retry queue: {len(self.retry_queue)}")
        
        if self.stats["games_processed"] > 0:
            success_rate = 100 * (1 - (self.stats["errors"] / (self.stats["games_processed"] + self.stats["errors"])))
            logger.info(f"Success rate: {success_rate:.2f}%")
        
        if duration.total_seconds() > 0:
            games_per_second = self.stats["games_processed"] / duration.total_seconds()
            logger.info(f"Processing rate: {games_per_second:.2f} games/second")
        
        logger.info("=" * 50)

def main():
    """Main entrypoint for running the ingestion process."""
    parser = argparse.ArgumentParser(description="NBA Data Ingestion Tool")
    parser.add_argument("--seasons", type=int, default=5, help="Number of seasons to fetch")
    parser.add_argument("--batch-size", type=int, default=10, help="Players per batch")
    parser.add_argument("--no-cache", action="store_true", help="Disable caching")
    parser.add_argument("--no-cleanup", action="store_true", help="Skip duplicate cleanup")
    parser.add_argument("--no-verify", action="store_true", help="Skip data verification")
    parser.add_argument("--no-retry-queue", action="store_true", help="Disable retry queue persistence")
    parser.add_argument("--max-retries", type=int, default=5, help="Maximum number of retries for failed requests")
    parser.add_argument("--timer-interval", type=int, default=60, help="Time between progress updates (seconds)")
    parser.add_argument("--start-player", type=int, default=0, help="Index of the first player to process (0-based)")
    parser.add_argument("--end-player", type=int, default=None, help="Index of the last player to process (exclusive)")
    args = parser.parse_args()
    
    # Create configuration from arguments
    config = {
        "seasons_to_fetch": args.seasons,
        "batch_size": args.batch_size,
        "enable_caching": not args.no_cache,
        "clean_duplicates": not args.no_cleanup,
        "verify_data": not args.no_verify,
        "retry_queue_persistence": not args.no_retry_queue,
        "max_retries": args.max_retries,
        "timer_interval": args.timer_interval
    }
    
    # Initialize and run ingestion
    ingestion = NBADataIngestion(config)
    ingestion.run_ingestion(start_player=args.start_player, end_player=args.end_player)

if __name__ == "__main__":
    main()

================
File: backend/scripts/init_db.py
================
import os
import sys
from pathlib import Path
from sqlalchemy import create_engine, inspect, text
from sqlalchemy.orm import sessionmaker
from sqlalchemy_utils import database_exists, create_database
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

script_dir = Path(__file__).resolve().parent  # .../backend/scripts
backend_dir = script_dir.parent               # .../backend
project_root = backend_dir.parent             # .../Sharpshooter Picks
sys.path.append(str(project_root))

from db_config import Base, engine  
from db_models.db_schema import Player, PlayerStats

def init_database():
    try:
        logger.info(f"Using database URL: {engine.url}")
        inspector = inspect(engine)
        tables_before = inspector.get_table_names()
        logger.info(f"Tables before drop: {tables_before}")
        
        logger.info("Dropping existing tables...")
        Base.metadata.drop_all(engine)

        inspector = inspect(engine)
        tables_after = inspector.get_table_names()
        logger.info(f"Tables after drop: {tables_after}")
        
        if not database_exists(engine.url):
            logger.info(f"Creating database at {engine.url}")
            create_database(engine.url)
        
        logger.info("Creating tables if they don't exist...")
        Base.metadata.create_all(engine)
        
        inspector = inspect(engine)
        tables = inspector.get_table_names()
        logger.info("Created tables: " + ", ".join(tables))
        
        Session = sessionmaker(bind=engine)
        session = Session()
        session.execute(text("SELECT 1"))
        session.close()
        
        logger.info("Database initialization completed successfully!")
        return True
        
    except Exception as e:
        logger.error(f"Error initializing database: {e}")
        return False

if __name__ == "__main__":
    if init_database():
        logger.info("Database is ready!")
    else:
        logger.error("Database initialization failed!")

================
File: backend/scripts/test_ingestion.py
================
import argparse
import random
import pandas as pd
import sys
from pathlib import Path
from typing import List, Dict

from sqlalchemy.orm import Session

script_dir = Path(__file__).resolve().parent  # .../backend/scripts
backend_dir = script_dir.parent               # .../backend
project_root = backend_dir.parent             # .../Sharpshooter Picks
sys.path.append(str(project_root))            #  the actual project root

from ingest_historical_stats import NBADataIngestion  # Same directory import
from db_config import engine  # From parent directory
from db_models.db_schema import Player, PlayerStats  # From parent's subdirectory

class TestNBAIngestion:
    
    def __init__(self):
        self.ingestion = NBADataIngestion()
        self.session = Session(engine)

        self.active_players = self.ingestion.get_active_players() or []
    
    def closeSession(self) -> None:
        """releases any resources tied up in the open connection"""
        self.session.close()
    
    def test_player_api(self) -> List[Dict]:
        """test nba-api player retrieval"""
        print("\n----- Testing NBA API Player Retrieval -----")
        
        if not self.active_players:
            print(" Failed to retrieve active players")
            return []
        
        print(f" Successfully retrieved {len(self.active_players)} active players")
        
        # Print five sample players to verify data structure
        print("\nSample player data:")
        for player in random.sample(self.active_players, 5):
            print(f"  {player['full_name']} (ID: {player['id']})")
                
        return self.active_players
    
    def test_game_api(self, player_name=None, season="2024-25") -> pd.Series:
        """Test the NBA API game retrieval for a specific player"""
        print(f"\n----- Testing NBA API Game Retrieval for Season {season} -----")

        player = None
        
        # If no player_id is provided, get one from test_player_api
        if player_name is None:
            print("Since no player name was specified in the command line arguments, I'm going to choose a random player from test_player_api() \n")

            # an empty list evaluates to false in python
            if not self.test_player_api():
                print(" Failed to retrieve players to test game API")
                return None


            player = random.choice(self.active_players)
            
            player_id = player['id']
            player_name = player['full_name']
            print(f"Selected player: {player['full_name']} (ID: {player_id})")
        else:
            player = next((p for p in self.active_players if p['full_name'].lower() == player_name.lower()), None)

            if player is None:
                print(f" Failed to find {player_name} in the database. Please make sure your spelling is correct, including accents and capitalization!")
                return None
            player_id = player['id']
                

        # Retrieve games for the player
        games_df = self.ingestion.get_player_games(player_id, season)
        
        if games_df is None or games_df.empty:
            print(f" Failed to retrieve games for {player_name} in season {season}")
            return None
        
        print(f" Successfully retrieved {len(games_df)} games for {player['full_name']}")
        

        print("\nAvailable columns:")
        print(", ".join(games_df.columns.tolist()))
        

        print("\nSample game data:")
        sample_game = games_df.iloc[random.randint(0, len(games_df) - 1)]
        
        relevant_cols = [
            'GAME_ID', 'GAME_DATE', 'MATCHUP', 'WL', 
            'MIN', 'PTS', 'REB', 'AST', 'STL', 'BLK', 
            'TOV', 'PLUS_MINUS', 'FGM', 'FGA', 'FG3M', 'FG3A', 
            'FTM', 'FTA'
        ]
        
        # only show columns that exist in the dataframe
        cols_to_show = [col for col in relevant_cols if col in sample_game.index]
        
        for col in cols_to_show:
            print(f"  {col}: {sample_game[col]}")
        
        return sample_game
    
    def test_process_game_data(self, player_name=None, season="2024-25") -> bool:
        """Test the game data processing functionality"""
        print("\n----- Testing Game Data Processing -----")


        # get a game
        sample_game = self.test_game_api(player_name=player_name, season="2024-25")

        if sample_game is None:
            print(" test_game_api() failed!")
            return False    
        
        # Process the game data
        processed_data = self.ingestion.process_game_data(sample_game, season)
        
        if processed_data is None:
            print(" Failed to process game data")
            return False
        
        print(" Successfully processed game data")
        print("\nProcessed game fields:")
        
        for key, value in processed_data.items():
            print(f"  {key}: {value}")
        
        # Verify all required fields are present
        expected_fields = [
            'game_id', 'player_id', 'game_date', 'season', 'is_home_game',
            'minutes_played', 'points', 'rebounds', 'assists', 'steals',
            'blocks', 'turnovers', 'plus_minus', 'fg_made', 'fg_attempted',
            'fg3_made', 'fg3_attempted', 'ft_made', 'ft_attempted'
        ]
        
        missing_fields = [field for field in expected_fields if field not in processed_data]
        
        if missing_fields:
            print(f" Missing fields in processed data: {', '.join(missing_fields)}")
            return False
        
        print(" All required fields are present in the processed data")
        return True
    
    def test_db_connection_and_schema(self) -> bool:
        """Test database connection and verify schema"""
        print("\n----- Testing Database Connection and Schema -----")
        
        try:
            # check if we can query the database
            player_count = self.session.query(Player).count()
            stats_count = self.session.query(PlayerStats).count()
            
            print(f" Database connection successful")
            print(f"  Current database contains {player_count} players and {stats_count} game stats records")
            
            # Check Player table columns
            player_columns = [column.name for column in Player.__table__.columns]
            print("\nPlayer table columns:", ", ".join(player_columns))
            
            # Check PlayerStats table columns
            stats_columns = [column.name for column in PlayerStats.__table__.columns]
            print("\nPlayerStats table columns:", ", ".join(stats_columns))
            
            return True
            
        except Exception as e:
            print(f" Database connection error: {e}")
            return False
    
    def test_single_player_ingestion(self, player_name=None, season="2024-25", limit_games=5) -> bool:
        """Test the ingestion process for a single player with limited games"""
        print("\n----- Testing Single Player Ingestion Process -----")

        # an empty list evaluates to false in python
        if not self.test_player_api():
            print(" Failed to retrieve players to test single player ingestion")
            return False
        
        player_id = None
        
        # Get a player to test
        if player_name is None:
            print("Since no player name was specified in the command line arguments, I'm going to choose a random player from test_player_api() \n")

            player = random.choice(self.active_players)
            
            player_name = player['full_name']
            print(f"Selected player: {player['full_name']} (ID: {player_id})")
        else:
            player = next((p for p in self.active_players if p['full_name'].lower() == player_name.lower()), None)

            if player is None:
                print(f" Failed to find {player_name} in the database. Please make sure your spelling is correct, including accents and capitalization!")
                return False
            
            print(f"Selected player: {player['full_name']} (ID: {player['id']})")
        
        player_id = player['id']

        print("\nStoring player in database...")
        self.ingestion.store_player_data(player)
        
        # Verify player was stored
        db_player = self.session.query(Player).filter_by(player_id=player_id).first()
        if not db_player:
            print(" Failed to store player in database")
            return False
        
        print(f" {db_player.full_name} stored successfully: ")
        
        # Get games for the player
        print(f"\n Retrieving games for player from season {season}...")
        games_df = self.ingestion.get_player_games(player_id, season)
        
        if games_df is None or games_df.empty:
            print(f" No games found for player ID {player_id} in season {season}")
            return False
        
        # head(<int n>) gives the first n rows 
        games_to_process = games_df.head(limit_games)
        print(f" Retrieved {len(games_df)} games, will process {len(games_to_process)}")
        
        # Process and store each game
        successful_games = 0
        # iterrows() returns a generator that yields tuples, where the first item is the row index, and the second item is the row data as a Pandas Series.
        for _, game in games_to_process.iterrows(): # _ indicates we don't need to index, which iterrows returns 
            try:
                processed_data = self.ingestion.process_game_data(game, season)
                if processed_data:
                    self.ingestion.store_game_stats(processed_data)
                    successful_games += 1
            except Exception as e:
                print(f" Error processing game: {e}")
        
        print(f" Successfully processed and stored {successful_games} out of {len(games_to_process)} games")
        
        # Verify games were stored
        db_stats = self.session.query(PlayerStats).filter_by(player_id=player_id).all()
        
        if not db_stats:
            print(" No game stats were stored in the database")
            return False
        
        print(f" Found {len(db_stats)} games in database for player {player_id}")
        
        
        if db_stats:
            print("\nSample of stored game data:")
            sample_stat = db_stats[0]
            print(f"  Game ID: {sample_stat.game_id}")
            print(f"  Date: {sample_stat.game_date}")
            print(f"  Points: {sample_stat.points}")
            print(f"  Rebounds: {sample_stat.rebounds}")
            print(f"  Assists: {sample_stat.assists}")
        
        return successful_games > 0
    
    def verify_data_in_tables(self, player_name) -> bool:
        """Verify the data in the database tables"""
        print("\n----- Verifying Data in Database Tables -----")
        
        # Get stats for a specific player if provided
        if player_name:
            player = self.session.query(Player).filter_by(full_name=player_name).first()
            if not player:
                print(f" {player_name} not found in database")
                return False
                        
            print(f"Found player: {player.full_name} (ID: {player.player_id})")
            
            stats = self.session.query(PlayerStats).filter_by(player_id=player.player_id).all()
            print(f"Found {len(stats)} game stats for this player")
            
            if stats:
                print("\nSeason breakdown:")
                stats_by_season = {}
                for stat in stats:
                    if stat.season not in stats_by_season:
                        stats_by_season[stat.season] = 0
                    stats_by_season[stat.season] += 1
                
                error = False

                for season, count in stats_by_season.items():
                    if count > 82:
                        print(f"  found {count} games in the {season} regular season. This is not possible!")
                        error = True
                    else:
                        print(f"  {season}: {count} games")

                if error:
                    print(f" error in verifying table data")
                    return False
        else:
            print(f" Please enter a player name! \n Add --player-name <PLAYER_NAME> to your arguments")
            return False
        
        return True
    
    def run_all_tests(self, player_name=None) -> None:
        """Run all tests"""
        print("=" * 60)
        print("RUNNING ALL NBA DATA INGESTION TESTS")
        print("=" * 60)
        
        # Define tests with their success conditions
        test_configs = [
            {
                "name": "Player API", 
                "func": self.test_player_api, 
                "args": [], 
                "success": lambda result: bool(result)
            },
            {
                "name": "Game API", 
                "func": self.test_game_api, 
                "args": [player_name], 
                "success": lambda result: result is not None
            },
            {
                "name": "Process Game Data", 
                "func": self.test_process_game_data, 
                "args": [player_name],
                "success": lambda result: bool(result)
            },
            {
                "name": "DB Connection", 
                "func": self.test_db_connection_and_schema, 
                "args": [], 
                "success": lambda result: bool(result)
            },
            {
                "name": "Single Player Ingestion", 
                "func": self.test_single_player_ingestion, 
                "args": [player_name], 
                "success": lambda result: bool(result)
            },
            {
                "name": "Verify Data", 
                "func": self.verify_data_in_tables, 
                "args": [player_name], 
                "success": lambda result: bool(result)
            }
        ]
        
        failed_tests = []

        success_count = 0
        
        for test in test_configs:
            try:
                # * is an unpacking operator. tells Python to unpack the list and pass each element as a separate argument
                result = test["func"](*test["args"]) # ex. test["func"] = self.test_game_api(player_name)
                if test["success"](result):
                    success_count += 1
                else:
                    failed_tests.append(test["name"])
            except Exception as e:
                print(f" Test failed with exception: {e}")
        
        print("\n" + "=" * 60)
        print(f"TEST SUMMARY: {success_count}/{len(test_configs)} tests passed")
        print("Failed tests: ")
        print([test for test in failed_tests])
        print("=" * 60)
        return None
    
if __name__ == "__main__":
    
    parser = argparse.ArgumentParser(description="Test the NBA data ingestion process")
    parser.add_argument("--player-name", type=str, help="Specific player to test with identified by name")
    parser.add_argument("--test", choices=["all", "playerAPI", "gameAPI", "process", "db", "ingestion", "verify"], 
                        default="all", help="Specific test to run")
    parser.add_argument("--season", type=str, default="2024-25", help="Season to use for testing")
    
    # note that --help / -h are built in
    args = parser.parse_args()
    
    tester = TestNBAIngestion()


    try:
        if args.test == "all":
            tester.run_all_tests(player_name=args.player_name)
        elif args.test == "playerAPI":
            tester.test_player_api()
        elif args.test == "gameAPI":
            tester.test_game_api(player_name=args.player_name, season=args.season)
        elif args.test == "process":
            tester.test_process_game_data(player_name=args.player_name, season=args.season)
        elif args.test == "db":
            tester.test_db_connection_and_schema()
        elif args.test == "ingestion":
            tester.test_single_player_ingestion(player_name=args.player_name, season=args.season)
        elif args.test == "verify":
            tester.verify_data_in_tables(player_name=args.player_name)
    finally:
        tester.closeSession()

================
File: backend/app.py
================
from flask import Flask, jsonify
from flask_cors import CORS # allows frontend to make request to the server
import requests
from datetime import datetime
import os
from dotenv import load_dotenv  
from ml.ml_model import NBAPredictor


load_dotenv()  # loads environment variables from .env file

app = Flask(__name__) # initialize Flask application, __name__ represents the name of the current module
CORS(app) # enables Cross-Origin Resource Sharing, allow requests from different origins


API_KEY = os.getenv('ODDS_API_KEY')
if not API_KEY: # if API_KEY is empty or falsy
    raise ValueError("Missing ODDS_API_KEY environment variable. Make sure you have a .env file with this value.")

BASE_URL = "https://api.the-odds-api.com/v4"

def calculate_confidence(odds):
    """
    Calculate confidence score based on betting odds.
    Returns a value between 0 and 1.
    """
    try:
        if odds > 0:
            probability = 100 / (odds + 100)
        else:
            probability = abs(odds) / (abs(odds) + 100)
        return round(probability, 3)
    except (TypeError, ZeroDivisionError):
        return None

# decorator, get_picks() will run when a HTTP request hits the server with the path "/api/picks", return value is sent back to client
@app.route("/api/picks", methods=["GET"]) # methods=["GET"] is not necssary since it is the default HTTP method, included for clarity
def get_picks():
    try:
        response = requests.get(
            # f-string
            f"{BASE_URL}/sports/basketball_nba/odds",
            params={
                "apiKey": API_KEY,
                "regions": "us",
                "markets": "h2h",
                "oddsFormat": "american"
            }
        )

        if response.status_code != 200: # 200 indicates a successful request
            print(f"API Error: {response.status_code}", response.text)
            # HTTP status code for the response, 500 indicates Internal Server Error
            return jsonify({"error": "Failed to fetch from Odds API"}), 500
            
        games = response.json()
        processed_games = []
        
        for idx, game in enumerate(games):
            try:
                # Skip games without odds data
                if not game.get('bookmakers'):
                    continue
                    
                bookmaker = game['bookmakers'][0]
                odds = bookmaker['markets'][0]['outcomes']
                
                # Find home and away team odds
                home_odds = None
                away_odds = None
                for odd in odds:
                    if odd['name'] == game['home_team']:
                        home_odds = odd['price']
                        break

                for odd in odds:
                    if odd['name'] == game['away_team']:
                        away_odds = odd['price']
                        break
                
                if home_odds is None or away_odds is None:
                    continue
                
                # Calculate confidence scores
                home_confidence = calculate_confidence(home_odds)
                away_confidence = calculate_confidence(away_odds)
                
                if home_confidence is None or away_confidence is None:
                    continue
                
                # Determine prediction
                if home_confidence > away_confidence:
                    predicted_winner = game['home_team']
                    confidence = home_confidence
                else:
                    predicted_winner = game['away_team']
                    confidence = away_confidence

                processed_games.append({
                    "id": str(idx + 1),
                    "team": game['home_team'],
                    "opponent": game['away_team'],
                    "prediction": f"{predicted_winner} Win",
                    "confidence": confidence,
                    "start_time": game['commence_time'],
                    "odds": {
                        "home_odds": home_odds,
                        "away_odds": away_odds
                    }
                })
                
            except Exception as e:
                print(f"Error processing game {idx}: {str(e)}")
                continue
        
        return jsonify(processed_games)

    except requests.exceptions.RequestException as e:
        print(f"Request error: {str(e)}")
        return jsonify({"error": "Failed to fetch game data"}), 500
    except Exception as e:
        print(f"Unexpected error: {str(e)}")
        return jsonify({"error": "An unexpected error occurred"}), 500
    

@app.route("/api/props", methods=["GET"])
def get_player_props():
    try:
        # Get all NBA games
        games_response = requests.get(
            f"{BASE_URL}/sports/basketball_nba/events",
            params={
                "apiKey": API_KEY
            }
        )
        
        if games_response.status_code != 200:
            print(f"Games API Error: {games_response.status_code}", games_response.text)
            return jsonify({"error": "Failed to fetch games"}), 500
            
        games = games_response.json()
        processed_props = []
        
        # For each game, get player props
        for game in games:
            try:
                props_response = requests.get(
                    f"{BASE_URL}/sports/basketball_nba/events/{game['id']}/odds",
                    params={
                        "apiKey": API_KEY,
                        "regions": "us",
                        "markets": "player_points,player_rebounds,player_assists",
                        "oddsFormat": "american",
                        "bookmakers": "draftkings"
                    }
                )
                
                if props_response.status_code != 200:
                    continue
                    
                props_data = props_response.json()
                
                # Process bookmaker data
                if not props_data.get('bookmakers'):
                    continue
                    
                bookmaker = props_data['bookmakers'][0]  # DraftKings
                
                for market in bookmaker['markets']:
                    for outcome in market['outcomes']:
                        confidence = calculate_confidence(outcome['price'])
                        if confidence is None:
                            continue
                            
                        processed_props.append({
                            "id": f"{game['id']}-{market['key']}-{outcome['name']}",
                            "game": f"{game['home_team']} vs {game['away_team']}",
                            "start_time": game['commence_time'],
                            "name": outcome['name'], 
                            "player": outcome['description'],
                            "market": market['key'].replace('player_', '').title(),
                            "line": outcome['point'],
                            "odds": outcome['price'],
                            "confidence": confidence
                        })
                        
            except Exception as e:
                print(f"Error processing props for game {game['id']}: {str(e)}")
                continue

        # Sort by confidence descending
        processed_props.sort(key=lambda x: x['confidence'], reverse=True)
        
        # Return top 10 most confident props
        return jsonify(processed_props[:10])

    except requests.exceptions.RequestException as e:
        print(f"Request error: {str(e)}")
        return jsonify({"error": "Failed to fetch prop data"}), 500
    except Exception as e:
        print(f"Unexpected error: {str(e)}")
        return jsonify({"error": "An unexpected error occurred"}), 500

if __name__ == "__main__": # starts the Flask Development Server
    app.run(debug=True, host='0.0.0.0')

================
File: backend/db_config.py
================
import os
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base, sessionmaker

DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://postgres:postgres@localhost:5432/nba_betting')

engine = create_engine(DATABASE_URL)
Base = declarative_base()

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

================
File: backend/dockerfile
================
# Use the official Python image
FROM python:3.9

# Set the working directory
WORKDIR /app

# Set Python path to include the current directory
ENV PYTHONPATH="${PYTHONPATH}:/app"

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the application code
COPY . .

# Expose the Flask default port
EXPOSE 5000

# Run the Flask app
CMD ["python", "-m", "flask", "run", "--host=0.0.0.0"]

================
File: backend/readme.md
================
## <u> backend </u>

### backend structure

backend/  
 models/  
    database.py        
 scripts/  
    ingest_historical_stats.py  
 visualizations/  
    outputs/          
    visualize_playerstats.py    
 app.py  
 dockerfile                 
 init_db.py                   
 requirements.txt     
<hr>
<br>
```database.py``` defines what data can be stored (table structures) -> ```db_init.py``` creates the actual database and tables -> ```ingestion.py``` uses ```database.py``` and ```db_init.py``` to fetch NBA data and stores it in the correct format.

<br>

Using the SQLAlchemy ORM (Object Relational Mapping) in ```database.py```, the application is able to translate between Python objects and database records, meaning the database records can be manipulated in Python.


<hr>


### <u>models/database.py </u>
defines the SQLAlchemy ORM models for the player stats database

ORM (Object-Relational Mapping) models allow us to interact with database records with Python objects without writing SQL queries

example:  
instead of 
```sql 
SELECT * FROM players WHERE player_id = 123;
```
we can write
```python
player = Player.query.filter_by(player_id=123).first()
```
<hr>

### <u>  ingest_historical_stats.py </u>
fetches and stores player statistics from the last 5 regular seasons making calls to [nba_api](https://github.com/swar/nba_api) (created by [swar](https://github.com/swar))

- comprehensive logging functionality
- batch processing with rate limiting  

#### Core components
##### NBADataIngestion

- Manages player data retrieval and processing
- Implements session management

##### Database Storage

- table ```players```: basic player information  
- table ```player_stats```: per-game statistics for each player


<hr>


### <u> init_db.py </u>
Initializes PostgreSQL database for the app

- Sets up required tables and schema
- database configuration verification
- detailed logging


#### Database Schema
<u>Players Table</u>

```properties
id: Primary key
player_id: Unique NBA ID
full_name: Player name
is_active: Active status
```


<u> PlayerStats Table </u>

```properties
id: Primary key
game_id: Game identifier
player_id: Foreign key to Players
game_date: DateTime
season: String
is_home_game: Boolean
minutes_played: String
points: Integer
assists: Integer
rebounds: Integer
steals: Integer
blocks: Integer
turnovers: Integer
plus_minus: Integer
fg_made: Integer
fg_attempted: Integer
fg3_made: Integer
fg3_attempted: Integer
ft_made: Integer
ft_attempted: Integer
```

<hr>

### Opening the PostgreSQL tables in Excel (for my own reference)
1. connect to the database psql in command line ```postgresql://localhost:5432/nba_betting```
2. \dt to view tables in the database
3. ```\copy (SELECT * FROM <table_name>) '~/Documents/Sharpshooter Picks/<table_name>.csv' CSV HEADER```
single quotations are used in the filepath because there's a space in the folder name
ex. ```\copy (SELECT * FROM players) '~/Documents/Sharpshooter Picks/players.csv' CSV HEADER```
ex. ```\copy (SELECT * FROM player_stats) '~/Documents/Sharpshooter Picks/player_stats.csv' CSV HEADER```




### viewing the database in Command Line
1. ```docker-compose up --build```
2. ```docker exec -it sharpshooterpicks-db-1 psql -U postgres -d nba_betting```
3. ```\dt```
4. ```SELECT * FROM players LIMIT 10;```
5. ```SEELCT * FROM player_stats LIMIT 10;```

to clear the tables: ```docker-compose exec backend python scripts/init_db.py```

================
File: backend/requirements.txt
================
requests>=2.26.0
Flask>=2.0.0
flask-cors>=4.0.0
python-dotenv>=0.19.0
pandas>=1.3.0
numpy>=1.21.0
typing>=3.7.4
torch>=2.0.0
nba_api==1.7.0
sqlalchemy>=2.0.0
psycopg2-binary>=2.9.0
SQLAlchemy-Utils>=0.41.0
tqdm>=4.67.1
backoff>=2.2.1

================
File: frontend/components/BettingDashboard.jsx
================
import React, { useState, useEffect } from 'react';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';
import { AlertCircle, TrendingUp, DollarSign } from 'lucide-react';

const BettingDashboard = () => {
  // State to hold betting data for different categories
  const [bets, setBets] = useState({
    overall: [],
    draftkings: [],
    fanduel: []
  });
  // Loading state to indicate if data is being fetched
  const [isLoading, setIsLoading] = useState(true);

  useEffect(() => {
    // Simulate fetching betting data (replace with real API calls)
    const fetchBets = async () => {
      try {
        setIsLoading(true);
        // Simulate API delay
        await new Promise(resolve => setTimeout(resolve, 1000));
        
        // Mock data for demonstration purposes
        const mockData = {
          overall: [
            {
              id: 1,
              game: "Celtics vs Lakers",
              pick: "Celtics -5.5",
              odds: -110,
              confidence: 0.85,
              time: "7:30 PM ET",
              book: "DraftKings"
            },
            {
              id: 2,
              game: "Warriors vs Suns",
              pick: "Over 235.5",
              odds: -105,
              confidence: 0.82,
              time: "10:00 PM ET",
              book: "FanDuel"
            }
          ],
          draftkings: [
            {
              id: 3,
              game: "Celtics vs Lakers",
              pick: "Celtics -5.5",
              odds: -110,
              confidence: 0.85,
              time: "7:30 PM ET"
            }
          ],
          fanduel: [
            {
              id: 4,
              game: "Warriors vs Suns",
              pick: "Over 235.5",
              odds: -105,
              confidence: 0.82,
              time: "10:00 PM ET"
            }
          ]
        };
        
        // Update state with the fetched (or mocked) data
        setBets(mockData);
      } catch (error) {
        console.error('Error fetching bets:', error);
      } finally {
        setIsLoading(false);
      }
    };

    fetchBets();
  }, []);

  // Component to render an individual bet card
  const BetCard = ({ bet }) => (
    <Card className="mb-4 hover:shadow-lg transition-shadow">
      <CardContent className="pt-6">
        {/* Header section with game info and confidence indicator */}
        <div className="flex justify-between items-start mb-4">
          <div>
            <h3 className="font-semibold text-lg text-gray-900">{bet.game}</h3>
            <p className="text-sm text-gray-500">{bet.time}</p>
          </div>
          <div className="text-right">
            {/* Confidence badge with color based on confidence level */}
            <div className={`inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium
              ${bet.confidence >= 0.8 ? 'bg-green-100 text-green-800' : 
                bet.confidence >= 0.7 ? 'bg-yellow-100 text-yellow-800' : 
                'bg-red-100 text-red-800'}`}>
              {(bet.confidence * 100).toFixed(0)}% Confidence
            </div>
            {/* Display bookmaker if available */}
            {bet.book && (
              <p className="text-sm text-gray-500 mt-1">{bet.book}</p>
            )}
          </div>
        </div>
        
        {/* Section with betting pick and odds */}
        <div className="flex justify-between items-center">
          <div className="flex items-center space-x-2">
            <TrendingUp className="w-4 h-4 text-gray-500" />
            <span className="font-medium">{bet.pick}</span>
          </div>
          <div className="flex items-center space-x-2">
            <DollarSign className="w-4 h-4 text-gray-500" />
            <span className={`font-medium ${bet.odds > 0 ? 'text-green-600' : 'text-red-600'}`}>
              {bet.odds > 0 ? `+${bet.odds}` : bet.odds}
            </span>
          </div>
        </div>
      </CardContent>
    </Card>
  );

  // Component to render content for a specific tab (list of bets)
  const TabContent = ({ bets }) => (
    <div className="space-y-4">
      {isLoading ? (
        // Display a spinner while loading data
        <div className="flex justify-center items-center h-64">
          <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-gray-900" />
        </div>
      ) : bets.length > 0 ? (
        // Map over bets and render a BetCard for each
        bets.map(bet => <BetCard key={bet.id} bet={bet} />)
      ) : (
        // Display a message when no bets are available
        <div className="text-center py-12">
          <AlertCircle className="mx-auto h-12 w-12 text-gray-400" />
          <h3 className="mt-2 text-sm font-medium text-gray-900">No bets available</h3>
          <p className="mt-1 text-sm text-gray-500">Check back later for new betting opportunities.</p>
        </div>
      )}
    </div>
  );

  return (
    <div className="max-w-4xl mx-auto p-4">
      <Card>
        <CardHeader>
          <CardTitle className="text-2xl font-bold">NBA Best Bets</CardTitle>
        </CardHeader>
        <CardContent>
          {/* Tabs to switch between overall bets and specific bookmakers */}
          <Tabs defaultValue="overall" className="w-full">
            <TabsList className="grid w-full grid-cols-3">
              <TabsTrigger value="overall">Overall Best Bets</TabsTrigger>
              <TabsTrigger value="draftkings">DraftKings</TabsTrigger>
              <TabsTrigger value="fanduel">FanDuel</TabsTrigger>
            </TabsList>
            <div className="mt-6">
              {/* Render the corresponding TabContent based on the selected tab */}
              <TabsContent value="overall">
                <TabContent bets={bets.overall} />
              </TabsContent>
              <TabsContent value="draftkings">
                <TabContent bets={bets.draftkings} />
              </TabsContent>
              <TabsContent value="fanduel">
                <TabContent bets={bets.fanduel} />
              </TabsContent>
            </div>
          </Tabs>
        </CardContent>
      </Card>
    </div>
  );
};

export default BettingDashboard;

================
File: frontend/components/BettingTabs.jsx
================
import { useState, useEffect } from 'react';
import * as Tabs from '@radix-ui/react-tabs';

export default function BettingTabs() {
    const [propsData, setPropsData] = useState([]);
    const [games, setGames] = useState([]);

    const [isLoadingProps, setIsLoadingProps] = useState(true);
    const [isLoadingGames, setIsLoadingGames] = useState(true);

    const [error, setError] = useState(null);

    useEffect(() => {
        const fetchProps = async () => {
            try {
                const response = await fetch('http://localhost:5001/api/props');
                if (!response.ok) throw new Error(`HTTP error! status: ${response.status}`);
                const data = await response.json();
                setPropsData(data);
            } catch (e) {
                setError('Failed to load props: ' + e.message);
            } finally {
                setIsLoadingProps(false);
            }
        };

        const fetchGames = async () => {
            try {
                const response = await fetch('http://localhost:5001/api/picks');
                if (!response.ok) throw new Error(`HTTP error! status: ${response.status}`);
                const data = await response.json();
                setGames(data);
            } catch (e) {
                setError('Failed to load games: ' + e.message);
            } finally {
                setIsLoadingGames(false);
            }
        };

        fetchProps();
        fetchGames();

        // Refresh data every 5 minutes
        const interval = setInterval(() => {
            fetchProps();
            fetchGames();
        }, 300000);

        return () => clearInterval(interval);
    }, []);

    const formatDate = (dateString) => {
        return new Date(dateString).toLocaleString('en-US', {
            weekday: 'short',
            month: 'short',
            day: 'numeric',
            hour: 'numeric',
            minute: '2-digit'
        });
    };

    const formatOdds = (odds) => (odds > 0 ? `+${odds}` : odds);

    return (
        <Tabs.Root defaultValue="games" className="w-full">
            {/* --- Tabs List --- */}
            <Tabs.List className="flex border-b border-gray-200">
                <Tabs.Trigger 
                    value="games" 
                    className="px-4 py-2 text-sm font-medium text-gray-600 hover:text-gray-900 cursor-pointer border-b-2 border-transparent data-[state=active]:border-blue-500 data-[state=active]:text-blue-600"
                >
                    Game Lines
                </Tabs.Trigger>
                <Tabs.Trigger 
                    value="props"
                    className="px-4 py-2 text-sm font-medium text-gray-600 hover:text-gray-900 cursor-pointer border-b-2 border-transparent data-[state=active]:border-blue-500 data-[state=active]:text-blue-600"
                >
                    Player Props
                </Tabs.Trigger>
            </Tabs.List>

            {/* --- GAMES TAB CONTENT --- */}
            <Tabs.Content value="games" className="mt-4">
                {isLoadingGames ? (
                    <div className="flex items-center justify-center h-64">
                        <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-gray-900"></div>
                    </div>
                ) : error ? (
                    <div className="text-red-500 p-4 rounded-lg bg-red-50">
                        {error}
                    </div>
                ) : (
                    <div className="space-y-4">
                        {games.map((game) => (
                            <div key={game.id} className="border rounded-lg p-4 hover:shadow-lg transition-shadow bg-white">
                                <div className="flex justify-between items-center mb-2">
                                    <div>
                                        <h3 className="text-lg font-medium">
                                            {game.team} vs {game.opponent}
                                        </h3>
                                        <p className="text-sm text-gray-500">
                                            {formatDate(game.start_time)}
                                        </p>
                                    </div>
                                    <div className="text-right">
                                        <span className={`text-lg font-bold ${
                                            game.confidence > 0.7 ? 'text-green-600' : 
                                            game.confidence > 0.5 ? 'text-yellow-600' : 
                                            'text-red-600'
                                        }`}>
                                            {(game.confidence * 100).toFixed(1)}%
                                        </span>
                                    </div>
                                </div>

                                <div className="mt-2 space-y-2">
                                    <div className="flex justify-between text-sm">
                                        <span className="text-gray-600">Prediction:</span>
                                        <span className="font-medium">{game.prediction}</span>
                                    </div>
                                    <div className="flex justify-between text-sm">
                                        <span className="text-gray-600">Odds:</span>
                                        <span>
                                            {game.team}: {formatOdds(game.odds.home_odds)} | {game.opponent}: {formatOdds(game.odds.away_odds)}
                                        </span>
                                    </div>
                                </div>
                            </div>
                        ))}

                        {games.length === 0 && (
                            <div className="text-gray-500 text-center py-8">
                                No games available at the moment
                            </div>
                        )}
                    </div>
                )}
            </Tabs.Content>

            {/* --- PROPS TAB CONTENT --- */}
            <Tabs.Content value="props" className="mt-4">
                {isLoadingProps ? (
                    <div className="flex items-center justify-center h-64">
                        <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-gray-900"></div>
                    </div>
                ) : error ? (
                    <div className="text-red-500 p-4 rounded-lg bg-red-50">
                        {error}
                    </div>
                ) : (
                    <div className="space-y-4">
                        {propsData.map((prop) => (
                            <div
                                key={prop.id}
                                className="border rounded-lg p-4 hover:shadow-lg transition-shadow bg-white"
                            >
                                <div className="flex justify-between items-center mb-2">
                                    <div>
                                        <h3 className="text-lg font-medium">{prop.player}</h3>
                                        <p className="text-sm text-gray-500">
                                            {prop.game}
                                        </p>
                                        <p className="text-sm text-gray-500">
                                            {formatDate(prop.start_time)}
                                        </p>
                                    </div>
                                    <div className="text-right">
                                        <span
                                            className={`text-lg font-bold ${
                                                prop.confidence > 0.7
                                                    ? 'text-green-600'
                                                    : prop.confidence > 0.5
                                                    ? 'text-yellow-600'
                                                    : 'text-red-600'
                                            }`}
                                        >
                                            {(prop.confidence * 100).toFixed(1)}%
                                        </span>
                                    </div>
                                </div>

                                <div className="mt-2 space-y-1 text-sm">
                                    <div className="flex justify-between">
                                        <span className='text-gray-600'>
                                            Over/Under:
                                        </span>
                                        <span className='font-medium'>
                                            {prop.name}
                                        </span> 
                                    </div>
                                    <div className="flex justify-between">
                                        <span className="text-gray-600">
                                            Market:
                                        </span>
                                        <span className="font-medium">
                                            {prop.market} ({prop.line})
                                        </span>
                                    </div>
                                    <div className="flex justify-between">
                                        <span className="text-gray-600">
                                            Odds:
                                        </span>
                                        <span>
                                            {formatOdds(prop.odds)}
                                        </span>
                                    </div>
                                </div>
                            </div>
                        ))}

                        {propsData.length === 0 && (
                            <div className="text-gray-500 text-center py-8">
                                No player props available at the moment
                            </div>
                        )}
                    </div>
                )}
            </Tabs.Content>
        </Tabs.Root>
    );
}

================
File: frontend/components/TopPicks.js
================
import { useState, useEffect } from 'react';


// Displays the top betting picks of the day as determined by the system
export default function TopPicks({ onGameSelect }) {
    const [games, setGames] = useState([]);
    const [isLoading, setIsLoading] = useState(true);
    const [error, setError] = useState(null);

    useEffect(() => {
        const fetchGames = async () => {
            try {
                const response = await fetch('http://localhost:5001/api/picks');
                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }
                const data = await response.json();
                setGames(data);
            } catch (e) {
                setError('Failed to load picks: ' + e.message);
            } finally {
                setIsLoading(false);
            }
        };

        fetchGames();
        // Refresh data every 5 minutes
        const interval = setInterval(fetchGames, 300000);
        return () => clearInterval(interval);
    }, []);

    const formatDate = (dateString) => {
        return new Date(dateString).toLocaleString('en-US', {
            weekday: 'short',
            month: 'short',
            day: 'numeric',
            hour: 'numeric',
            minute: '2-digit'
        });
    };

    // Format odds for display
    const formatOdds = (odds) => {
        return odds > 0 ? `+${odds}` : odds;
    };

    if (isLoading) {
        return (
            <div className="flex items-center justify-center h-64">
                <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-gray-900"></div>
            </div>
        );
    }

    if (error) {
        return (
            <div className="text-red-500 p-4 rounded-lg bg-red-50">
                {error}
            </div>
        );
    }

    return (
        <div className="space-y-4">
            {games.map((game) => (
                <div
                    key={game.id}
                    className="border rounded-lg p-4 hover:shadow-lg transition-shadow cursor-pointer bg-white"
                    onClick={() => onGameSelect(game)}
                >
                    {/* Game Header */}
                    <div className="flex justify-between items-center mb-2">
                        <div>
                            <h3 className="text-lg font-medium">
                                {game.team} vs {game.opponent}
                            </h3>
                            <p className="text-sm text-gray-500">
                                {formatDate(game.start_time)}
                            </p>
                        </div>
                        <div className="text-right">
                            <span className={`text-lg font-bold ${
                                game.confidence > 0.7 ? 'text-green-600' : 
                                game.confidence > 0.5 ? 'text-yellow-600' : 
                                'text-red-600'
                            }`}>
                                {(game.confidence * 100).toFixed(1)}%
                            </span>
                        </div>
                    </div>

                    {/* Prediction & Odds */}
                    <div className="mt-2 space-y-2">
                        <div className="flex justify-between text-sm">
                            <span className="text-gray-600">Prediction:</span>
                            <span className="font-medium">{game.prediction}</span>
                        </div>
                        <div className="flex justify-between text-sm">
                            <span className="text-gray-600">Odds:</span>
                            <span>
                                {game.team}: {formatOdds(game.odds.home)} | {game.opponent}: {formatOdds(game.odds.away)}
                            </span>
                        </div>
                    </div>
                </div>
            ))}

            {games.length === 0 && (
                <div className="text-gray-500 text-center py-8">
                    No games available at the moment
                </div>
            )}
        </div>
    );
}

================
File: frontend/pages/_app.js
================
import '../styles/globals.css';

function MyApp({ Component, pageProps }) {
  return <Component {...pageProps} />;
}

export default MyApp;

================
File: frontend/pages/index.js
================
import BettingDashboard from '../components/BettingDashboard';

export default function Home() {
    return (
        <div className="min-h-screen bg-gray-50">
            <header className="bg-white shadow">
                <div className="max-w-7xl mx-auto py-6 px-4">
                    <h1 className="text-3xl font-bold text-gray-900">
                        NBA Betting Dashboard
                    </h1>
                </div>
            </header>

            <main className="max-w-7xl mx-auto py-6 sm:px-6 lg:px-8">
                <BettingDashboard />
            </main>
        </div>
    );
}

================
File: frontend/styles/globals.css
================
@tailwind base;
@tailwind components;
@tailwind utilities;

================
File: frontend/dockerfile
================
# Use Node.js as our base image
FROM node:18-alpine

# Set working directory in the container
WORKDIR /app

# Copy package files first (this helps with caching)
COPY package*.json ./

# Install dependencies
RUN npm install

# Copy the rest of the application
COPY . .

# Expose the port Next.js runs on
EXPOSE 3000

# Start the development server
CMD ["npm", "run", "dev"]

================
File: frontend/package.json
================
{
  "name": "frontend",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "dependencies": {
    "@radix-ui/react-slot": "^1.1.1",
    "@radix-ui/react-tabs": "^1.1.2",
    "@shadcn/ui": "^0.0.4",
    "autoprefixer": "^10.4.20",
    "class-variance-authority": "^0.7.1",
    "clsx": "^2.1.1",
    "lucide-react": "^0.471.1",
    "next": "^15.1.4",
    "postcss": "^8.5.1",
    "react": "^19.0.0",
    "react-dom": "^19.0.0",
    "tailwindcss": "^3.4.17",
    "tailwindcss-animate": "^1.0.7"
  }
}

================
File: frontend/postcss.config.js
================
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}

================
File: frontend/tailwind.config.js
================
/** @type {import('tailwindcss').Config} */
module.exports = {
    content: [
      "./app/**/*.{js,ts,jsx,tsx,mdx}",
      "./pages/**/*.{js,ts,jsx,tsx,mdx}",
      "./components/**/*.{js,ts,jsx,tsx,mdx}",
    ],
    theme: {
      extend: {},
    },
    plugins: [],
  }

================
File: notes/testing/nba_api_sandbox.py
================
from nba_api.stats.static import players
from nba_api.stats.endpoints import leaguegamefinder
import pandas as pd
import random

# Step 1: get active players and select one
active_players = players.get_active_players()

king_james = [player for player in active_players if player['full_name'] == "Jayson Tatum"][0]
player_id = king_james['id']
print(f"Selected Player: {king_james['full_name']} (ID: {player_id})")

# Step 2: retrieve games for the player

seasons = [
   "2024-25",
   "2023-24",
   "2022-23", 
   "2021-22",
   "2020-21"
]

for season in seasons:

    gamefinder = leaguegamefinder.LeagueGameFinder(
        player_or_team_abbreviation="P",
        player_id_nullable=player_id,
        season_type_nullable="Regular Season",
        season_nullable=season
    ) 

    # converts gamefinder into a pandas dataframe
    games_df = gamefinder.get_data_frames()[0]

    print(games_df.head()) # first 5 rows
    print(games_df.columns) # gives column labels (ex. plyer names, points scored)
    print(games_df.index) # gives row index
    print(games_df.index.tolist()) 

    season_id = games_df['SEASON_ID'].iloc[0]  # '22019'
    year = int(season_id[1:])  # 2019
    formatted_year = f"{year}-{str(year+1)[2:]}" # '2019-20'

    print(f"Found {len(games_df)} games for {king_james['full_name']} during the {formatted_year} regular season.")

minimum_season = 0
max_season = len(seasons) - 1
random_season = seasons[random.randint(minimum_season, max_season)]

print(random_season)

gamefinder = leaguegamefinder.LeagueGameFinder(
    player_or_team_abbreviation="P",
    player_id_nullable=player_id,
    season_type_nullable="Regular Season",
    season_nullable=random_season
)

games_df = gamefinder.get_data_frames()[0]

cols_to_describe = [col for col in games_df.columns if col != "TEAM_ID"]
print(games_df[cols_to_describe].describe())

print(games_df['MIN'])
print(games_df.iloc[1]) # (integer location) accesses the second row

# games_df = gamefinder.get_data_frames()[0]

# minimum_game = 0
# maximum_game = len(games_df) - 1 # number of total games played that season
# random_game = games_df.iloc[random.randint(minimum_game, maximum_game)]
# print(random_game)

================
File: notes/nba_api.md
================
![package structure](images/image.png)


##### finding all active players
- from nba_api.stats.static import players
- players.get_active_players()
```JSON
    player = {
        'id': player_id,
        'full_name': full_name,
        'first_name': first_name,
        'last_name': last_name,
        'is_active': True or False
    }
``` 

##### Find every game a player has ever played
```python
gamefinder = leaguegamefinder.LeagueGameFinder(
    player_or_team_abbreviation="P",
    player_id_nullable=player_id,
    season_type_nullable=regular,
    season_nullable="2019-20"
)   
```

##### Get a list of games only within the last 5 seasons


#### BoxScore for a particular game
##### nba_api -> live -> endpoints
- [BoxScore](https://github.com/swar/nba_api/blob/master/docs/nba_api/live/endpoints/boxscore.md)


###### nba_api -> stats -> endpoints
- [CumeStatsPlayerGames](https://github.com/swar/nba_api/blob/master/docs/nba_api/stats/endpoints/cumestatsplayergames.md)
- [LeagueGameFinder](https://github.com/swar/nba_api/blob/master/docs/nba_api/stats/endpoints/leaguegamefinder.md)
can be used to retrieve every game a player has played 
    ```python
    gamefinder = leaguegamefinder.LeagueGameFinder(
        player_or_team_abbreviation="P",
        player_id_nullable=player_id
    )   
    ```


.get_data_frames() 
- helper function provided by the API

================
File: notes/pandas.md
================
pandas.DataFrame.describe()
- 

pandas.DataFrame.columns
- gives column labels

pandas.DataFrame.index 
- gives row labels


pandas.DataFrame[<column-label>] 
- returns the entire column as a Series

pandas.DataFrame.iloc[int]
- short for integer location
- returns the row associated with that index

pandas.DataFrame[<column-label>].iloc[int]
- returns the item at [<column-label>, int]

pandas.DataFrame.iloc[int, int]
- purely positional indexing

================
File: notes/todo.txt
================
Complete the ML Model Implementation


Connect Backend Endpoints to ML Model

Connect Frontend to Real API

Fix Database Connection Issues


display bookmaker odds vs. SharpshooterPicks calculated odds

discssuion forum where users can discuss bets

================
File: .env.example
================
# Instead of your actual API key
ODDS_API_KEY=your_api_key_here

================
File: .gitignore
================
# Dependencies
node_modules/
/.pnp
.pnp.js

# Next.js
/frontend/.next/
/out/

# Production
/build

# Environment files
.env
.env.local
.env.development.local
.env.test.local
.env.production.local
.env*.local

# Debug logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store

# Python
__pycache__/
*.py[cod]
*$py.class
venv/
.env/
*.pyc
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Docker
.docker/
docker-compose.override.yml

# Coverage and test reports
.coverage
htmlcov/
.pytest_cache/
coverage/
.nyc_output

# Database
*.sqlite3
*.db

# Logs
logs/
*.log

# Cache files
backend/scripts/cache/

# System Files
.DS_Store
Thumbs.db

repomix-output.txt

================
File: docker-compose.yml
================
services:
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    depends_on:
      - backend
    environment:
      - NODE_ENV=development
      - CHOKIDAR_USEPOLLING=true  # Enable polling for file changes
    volumes:
      - ./frontend:/app           # Mount the frontend source code
      - /app/node_modules         # Prevent overwriting node_modules
    command: npm run dev          # Ensure the dev server is running

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    environment:
      - FLASK_APP=app.py
      - FLASK_ENV=development
      - ODDS_API_KEY=${ODDS_API_KEY}  # Uses value from .env file
    ports:
      - "5001:5000"
    depends_on:
      - db
    volumes:
      - ./backend:/app            # Mount the backend source code
      - /app/venv                 # If using a virtual environment

  db:
    image: postgres:14
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - db_data:/var/lib/postgresql/data

volumes:
  db_data:

================
File: readme.md
================
# Welcome to Sharpshooter Picks! 
<hr>

## Description:
**Sharpshooter Picks**  is a full stack web-app built for selecting the **best NBA player proposition bets** using **Machine Learning**.
Models are trained on [nba_api](https://github.com/swar/nba_api)  and picks are found using [odds-api](https://the-odds-api.com/).


Users can also individually choose from the most popular bookmakers, including DraftKings, Fanduel, and BetMGM

<hr>

##  Tech Stack

### Frontend

- Framework: Next.js (React)
- UI Components:
  - shadcn/ui components
  - Radix UI primitives
  - Lucide React icons


- Styling: Tailwind CSS
- Data Fetching: Native fetch API with interval updates

### Backend

- Framework: Flask (Python)
- Database: PostgreSQL with SQLAlchemy ORM
- Data Sources:
  - NBA API for historical player statistics
  - The Odds API for live betting odds


- Machine Learning: PyTorch

- Key Libraries:
  - pandas
  - numpy
  - requests
  - python-dotenv



### DevOps

- Containerization: Docker / Docker Compose
- Repomix (converts the entire project into a .txt file)
    - from the root directory, run ```repomix --ignore "frontend/node_modules/**,frontend/.next/**,player_stats.csv,players.csv"```

<hr>


##  Data Pipeline

![SharpshooterPicks Data Pipeline](notes/images/SharpshooterPicks-data-pipeline.drawio.png)

1. Gather all currently active players from nba_api (One time)
2. For each player, gather stats for every game for that player within the last 5 years (One time)
3. Machine learning model training (One time)
4. Real time fetching of  player proposition bets for the day from all bookmakers listed in odds-api that support player prop betting 
5. Confidence score calculation, calculates which bets have the highest chance of hitting
6. Frontend displays bets with the highest confidence score and updates
